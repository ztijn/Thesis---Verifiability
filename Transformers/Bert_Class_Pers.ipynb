{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"Bert_Class_Pers.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"6eb8b413d54d4854a3a34645296588d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3be68f72683d434885de6ab703a31a21","IPY_MODEL_fa5cbe9e25cf45b7a9e3ed3e12c446fd","IPY_MODEL_4c660217cc7e4eeeb62e6fca083cd7f1"],"layout":"IPY_MODEL_303ff644909b4d5b8617ac4f8446a1b0"}},"3be68f72683d434885de6ab703a31a21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3c43a5146fb42d7b46055a82201a2fa","placeholder":"​","style":"IPY_MODEL_ccfff87565a44d9c8be2c21537f41f71","value":"Downloading: 100%"}},"fa5cbe9e25cf45b7a9e3ed3e12c446fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4aeed19586644a8aecd6733fc2b368a","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9aee2c5f125c43d69e44b01104afaa00","value":231508}},"4c660217cc7e4eeeb62e6fca083cd7f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_866483d0cd5244898246dcc71aaa11cb","placeholder":"​","style":"IPY_MODEL_6b1dcbeb2cbe4ee491d7b0cc646ae23f","value":" 226k/226k [00:00&lt;00:00, 256kB/s]"}},"303ff644909b4d5b8617ac4f8446a1b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3c43a5146fb42d7b46055a82201a2fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccfff87565a44d9c8be2c21537f41f71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d4aeed19586644a8aecd6733fc2b368a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9aee2c5f125c43d69e44b01104afaa00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"866483d0cd5244898246dcc71aaa11cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b1dcbeb2cbe4ee491d7b0cc646ae23f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3dda60528f94f08b3505405ec9c2f08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47b6bdedd21d438b8b3a1fabf978d2fc","IPY_MODEL_50eb232a726944efa57cc6b7adaa0b1e","IPY_MODEL_137a7410663343a88cbb6d113101e1d3"],"layout":"IPY_MODEL_b43eee8210ea4808b4d9206780e83aa9"}},"47b6bdedd21d438b8b3a1fabf978d2fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33c67d4926ad4d24a6e9a17c2bda8887","placeholder":"​","style":"IPY_MODEL_dfdf5ee1c6734c16ac6da6011cb1add4","value":"Downloading: 100%"}},"50eb232a726944efa57cc6b7adaa0b1e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce54fe8b608f4c089e99f1a6662de842","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a8104e94b65485c83ccaba9404b887e","value":28}},"137a7410663343a88cbb6d113101e1d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df03be0456604f5e97d4bed51c639dcf","placeholder":"​","style":"IPY_MODEL_0c4d2ac843054785a4adb070f5ec62b4","value":" 28.0/28.0 [00:00&lt;00:00, 746B/s]"}},"b43eee8210ea4808b4d9206780e83aa9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33c67d4926ad4d24a6e9a17c2bda8887":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfdf5ee1c6734c16ac6da6011cb1add4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce54fe8b608f4c089e99f1a6662de842":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a8104e94b65485c83ccaba9404b887e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df03be0456604f5e97d4bed51c639dcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c4d2ac843054785a4adb070f5ec62b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d6b221e607c42938cbc7a2d90fc4671":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b503c36131540f99cfde750f5576139","IPY_MODEL_9ac880e876d046d9a86a53e3c296efe0","IPY_MODEL_5563966a264b4538abb389f1da9193f9"],"layout":"IPY_MODEL_0f65585841b645c09573cbf5c42546e3"}},"4b503c36131540f99cfde750f5576139":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cbd58c8f4df46a7a3432b944d51a396","placeholder":"​","style":"IPY_MODEL_18157336de114b8b97ea00f16bfaf96a","value":"Downloading: 100%"}},"9ac880e876d046d9a86a53e3c296efe0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b8ef27c09174e44938d0d28356f1a4f","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c6600fe7ad624e4ea49aaf11029dde3f","value":570}},"5563966a264b4538abb389f1da9193f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2de04c73d5e147b4bc15ddffa1add282","placeholder":"​","style":"IPY_MODEL_6c93b0d4bccd4485950e110c60aff5b8","value":" 570/570 [00:00&lt;00:00, 15.8kB/s]"}},"0f65585841b645c09573cbf5c42546e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cbd58c8f4df46a7a3432b944d51a396":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18157336de114b8b97ea00f16bfaf96a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b8ef27c09174e44938d0d28356f1a4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6600fe7ad624e4ea49aaf11029dde3f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2de04c73d5e147b4bc15ddffa1add282":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c93b0d4bccd4485950e110c60aff5b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"54debfe60a7c48f48bd83b225b71f5e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5006520cd5f4591bb32c6529608c0b0","IPY_MODEL_5bd3caffdbed437e83ee055f548a9102","IPY_MODEL_1171c2f6e82944538505aaf4029bf225"],"layout":"IPY_MODEL_728cdef76ffe48f7a1b5007a5f324798"}},"a5006520cd5f4591bb32c6529608c0b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_685efb6ae64946c09349c3b7cad13e67","placeholder":"​","style":"IPY_MODEL_5162dc05d7c84d9299bfab229bdc7cd5","value":"Downloading: 100%"}},"5bd3caffdbed437e83ee055f548a9102":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ebf74c395ab446fa73c0f4d5242a619","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d66f8f52b3c44507947d6ebe42e6ef2c","value":440473133}},"1171c2f6e82944538505aaf4029bf225":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee6058d0385f47c08dc37b724420726e","placeholder":"​","style":"IPY_MODEL_a1471e991081421e82d6b9b6961f18a2","value":" 420M/420M [00:07&lt;00:00, 58.8MB/s]"}},"728cdef76ffe48f7a1b5007a5f324798":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"685efb6ae64946c09349c3b7cad13e67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5162dc05d7c84d9299bfab229bdc7cd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ebf74c395ab446fa73c0f4d5242a619":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d66f8f52b3c44507947d6ebe42e6ef2c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee6058d0385f47c08dc37b724420726e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1471e991081421e82d6b9b6961f18a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["#Sentence Classification using BERT"],"metadata":{"id":"EKOTlwcmxmej"}},{"cell_type":"code","source":["import tensorflow as tf\n","# Verifying GPU availability (you have to turn it on in google colab)\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"metadata":{"id":"DEfSbAA4QHas","outputId":"dce3f6db-9240-464c-9889-3bd51bde7c84","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198138558,"user_tz":-120,"elapsed":6972,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\")"],"metadata":{"id":"oYsV4H8fCpZ-","trusted":true,"executionInfo":{"status":"ok","timestamp":1657198144339,"user_tz":-120,"elapsed":4299,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"0NmMdkZO8R6q","outputId":"0603c2f3-20a0-468a-8bcf-e0a7d5276ae6","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198157998,"user_tz":-120,"elapsed":12420,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 9.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 47.8 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 46.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_UkeC7SG2krJ","outputId":"7f57af37-f040-4a56-d5a4-eb512bacd57c","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198187051,"user_tz":-120,"elapsed":27309,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Loading csv file as df and removing irrelevant / empty (= multiple frames) frames\n","train_df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/train_set_verif.csv')\n","validation_df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/val_set_verif.csv')"],"metadata":{"id":"wj2xpqX7exO1","executionInfo":{"status":"ok","timestamp":1657198195651,"user_tz":-120,"elapsed":2618,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(train_df.shape)\n","print(validation_df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hgs15asBe_Fr","executionInfo":{"status":"ok","timestamp":1657198197257,"user_tz":-120,"elapsed":2,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"bd584be8-83f4-48c9-ebdf-f4eb82ebcd84"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(1870, 13)\n","(330, 13)\n"]}]},{"cell_type":"code","source":["from sklearn import preprocessing\n","\n","# Creating list of comments and corresponding labels\n","sentences_train = train_df.sentence.values\n","sentences_val = validation_df.sentence.values\n","\n","# Getting labels           \n","labels_train = train_df.personal.values\n","labels_val = validation_df.personal.values\n","\n","# Getting unique labels \n","un_labels = set()\n","for label in labels_train:\n","  un_labels.add(label)\n","print(len(un_labels), un_labels)\n","# Rewriting labels from str to int so it can be 'understood' by the classifier \n","le = preprocessing.LabelEncoder()\n","le.fit(sorted(un_labels))\n","\n","train_labels = le.transform(labels_train)\n","validation_labels = le.transform(labels_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9Bzi-owfIcJ","executionInfo":{"status":"ok","timestamp":1657198252023,"user_tz":-120,"elapsed":665,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"356f3f22-6274-4062-a087-dae96977dee8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["2 {'NonPers', 'Pers'}\n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"id":"Z474sSC6oe7A","trusted":true,"executionInfo":{"status":"ok","timestamp":1657198267641,"user_tz":-120,"elapsed":8847,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["6eb8b413d54d4854a3a34645296588d2","3be68f72683d434885de6ab703a31a21","fa5cbe9e25cf45b7a9e3ed3e12c446fd","4c660217cc7e4eeeb62e6fca083cd7f1","303ff644909b4d5b8617ac4f8446a1b0","b3c43a5146fb42d7b46055a82201a2fa","ccfff87565a44d9c8be2c21537f41f71","d4aeed19586644a8aecd6733fc2b368a","9aee2c5f125c43d69e44b01104afaa00","866483d0cd5244898246dcc71aaa11cb","6b1dcbeb2cbe4ee491d7b0cc646ae23f","a3dda60528f94f08b3505405ec9c2f08","47b6bdedd21d438b8b3a1fabf978d2fc","50eb232a726944efa57cc6b7adaa0b1e","137a7410663343a88cbb6d113101e1d3","b43eee8210ea4808b4d9206780e83aa9","33c67d4926ad4d24a6e9a17c2bda8887","dfdf5ee1c6734c16ac6da6011cb1add4","ce54fe8b608f4c089e99f1a6662de842","6a8104e94b65485c83ccaba9404b887e","df03be0456604f5e97d4bed51c639dcf","0c4d2ac843054785a4adb070f5ec62b4","1d6b221e607c42938cbc7a2d90fc4671","4b503c36131540f99cfde750f5576139","9ac880e876d046d9a86a53e3c296efe0","5563966a264b4538abb389f1da9193f9","0f65585841b645c09573cbf5c42546e3","1cbd58c8f4df46a7a3432b944d51a396","18157336de114b8b97ea00f16bfaf96a","3b8ef27c09174e44938d0d28356f1a4f","c6600fe7ad624e4ea49aaf11029dde3f","2de04c73d5e147b4bc15ddffa1add282","6c93b0d4bccd4485950e110c60aff5b8"]},"outputId":"5e8b21f6-92ee-4854-d4f3-1b4b77a1fc3e"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eb8b413d54d4854a3a34645296588d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3dda60528f94f08b3505405ec9c2f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d6b221e607c42938cbc7a2d90fc4671"}},"metadata":{}}]},{"cell_type":"code","source":["def sent_to_id(sentences):\n","  # Tokenize all of the sentences and map the tokens to thier word IDs.\n","  input_ids = []\n","\n","  # For every sentence...\n","  for sent in sentences:\n","      # `encode` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      encoded_sent = tokenizer.encode(\n","                          sent\n","                    )\n","      \n","      # Add the encoded sentence to the list.\n","      input_ids.append(encoded_sent)\n","\n","  # Print sentence 0, now as a list of IDs.\n","  print('Original: ', sentences[0])\n","  print('Token IDs:', input_ids[0])\n","  return(input_ids)"],"metadata":{"id":"2bBdb3pt8LuQ","trusted":true,"executionInfo":{"status":"ok","timestamp":1657198275781,"user_tz":-120,"elapsed":415,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["train_inputs = sent_to_id(sentences_train)\n","validation_inputs = sent_to_id(sentences_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Yj-NSQ8gGEX","executionInfo":{"status":"ok","timestamp":1657198279340,"user_tz":-120,"elapsed":1521,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"771f3ae0-d067-42db-b619-a93a73a69e06"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  Most of the people in America don't see a dime of social program money until they turn 65, but they start contributing to them at 16 (or whenever they get their first job).\n","Token IDs: [101, 2087, 1997, 1996, 2111, 1999, 2637, 2123, 1005, 1056, 2156, 1037, 27211, 1997, 2591, 2565, 2769, 2127, 2027, 2735, 3515, 1010, 2021, 2027, 2707, 8020, 2000, 2068, 2012, 2385, 1006, 2030, 7188, 2027, 2131, 2037, 2034, 3105, 1007, 1012, 102]\n","Original:  You don't make any attempt to cite your claims.\n","Token IDs: [101, 2017, 2123, 1005, 1056, 2191, 2151, 3535, 2000, 21893, 2115, 4447, 1012, 102]\n"]}]},{"cell_type":"code","source":["print('Max sentence length(train): ', max([len(sen) for sen in train_inputs]))\n","print('Max sentence length(validation): ', max([len(sen) for sen in validation_inputs]))"],"metadata":{"id":"JhUZO9vc_l6T","outputId":"472bf179-05bb-426a-d994-334ad2ab8e12","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198283938,"user_tz":-120,"elapsed":383,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sentence length(train):  122\n","Max sentence length(validation):  137\n"]}]},{"cell_type":"code","source":["# We will use some utility function from tensorflow(Tensorflow was my first crush)\n","from keras.preprocessing.sequence import pad_sequences\n","\n","MAX_LEN = 305\n","\n","#Padding the input to the max length that is 64\n","train_inputs = pad_sequences(train_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","validation_inputs = pad_sequences(validation_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")"],"metadata":{"id":"Cp9BPRd1tMIo","trusted":true,"executionInfo":{"status":"ok","timestamp":1657198290916,"user_tz":-120,"elapsed":1034,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def attentionmasks(input_ids):\n","  # Creating the attention masks\n","  attention_masks = []\n","\n","  # For each sentence...\n","  for sent in input_ids:\n","      \n","      # Create the attention mask.\n","      #   - If a token ID is 0, then it's padding, set the mask to 0.\n","      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","      att_mask = [int(token_id > 0) for token_id in sent]\n","      \n","      # Store the attention mask for this sentence.\n","      attention_masks.append(att_mask)\n","  return(attention_masks)"],"metadata":{"id":"cDoC24LeEv3N","trusted":true,"executionInfo":{"status":"ok","timestamp":1657198293847,"user_tz":-120,"elapsed":424,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["train_masks = attentionmasks(train_inputs)\n","validation_masks = attentionmasks(validation_inputs)"],"metadata":{"id":"-l_lHhwGiSiX","executionInfo":{"status":"ok","timestamp":1657198296185,"user_tz":-120,"elapsed":494,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["#Converting the input data to the tensor , which can be feeded to the model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"metadata":{"id":"jw5K2A5Ko1RF","trusted":true,"executionInfo":{"status":"ok","timestamp":1657198298753,"user_tz":-120,"elapsed":420,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#Creating the DataLoader which will help us to load data into the GPU/CPU\n","batch_size = 32\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"],"metadata":{"id":"GEgLpFVlo1Z-","trusted":true,"executionInfo":{"status":"ok","timestamp":1657198305039,"user_tz":-120,"elapsed":391,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["#Loading the pre-trained BERT model from huggingface library\n","\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", \n","    num_labels = 2,   \n","    output_attentions = False, \n","    output_hidden_states = False, )\n","\n","# Teeling the model to run on GPU\n","model.cuda()"],"metadata":{"id":"gFsCTp_mporB","outputId":"7b95f6ec-8545-4fc1-a91d-5a24af1a118d","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["54debfe60a7c48f48bd83b225b71f5e2","a5006520cd5f4591bb32c6529608c0b0","5bd3caffdbed437e83ee055f548a9102","1171c2f6e82944538505aaf4029bf225","728cdef76ffe48f7a1b5007a5f324798","685efb6ae64946c09349c3b7cad13e67","5162dc05d7c84d9299bfab229bdc7cd5","7ebf74c395ab446fa73c0f4d5242a619","d66f8f52b3c44507947d6ebe42e6ef2c","ee6058d0385f47c08dc37b724420726e","a1471e991081421e82d6b9b6961f18a2"]},"executionInfo":{"status":"ok","timestamp":1657198334438,"user_tz":-120,"elapsed":22281,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54debfe60a7c48f48bd83b225b71f5e2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# AdamW is an optimizer which is a Adam Optimzier with weight-decay-fix\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, \n","                  eps = 1e-8 \n","                )\n"],"metadata":{"id":"GLs72DuMODJO","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198338595,"user_tz":-120,"elapsed":376,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"dc73b1ee-4899-444c-b425-a470fe437900"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","scheduler"],"metadata":{"id":"-p0upAhhRiIx","outputId":"6899168b-8f69-4298-a432-cc42df42a27e","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198342892,"user_tz":-120,"elapsed":385,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.optim.lr_scheduler.LambdaLR at 0x7f52c7946b90>"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["Define a helper function for calculating accuracy."],"metadata":{"id":"pE5B99H5H2-W"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"9cQNvaZ9bnyy","trusted":true,"executionInfo":{"status":"ok","timestamp":1657198349352,"user_tz":-120,"elapsed":394,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#Creating the helper function to have a watch on elapsed time\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"metadata":{"id":"gpt6tR83keZD","trusted":true,"executionInfo":{"status":"ok","timestamp":1657198352802,"user_tz":-120,"elapsed":391,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["#Let's start the training process\n","\n","import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"metadata":{"id":"6J-FYdx6nFE_","outputId":"c1128fb2-6596-4abd-b65f-0d62de72c8ef","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198784593,"user_tz":-120,"elapsed":424114,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:01:07.\n","\n","  Average training loss: 0.33\n","  Training epoch took: 0:01:38\n","\n","Running Validation...\n","  Accuracy: 0.96\n","  Validation took: 0:00:07\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:01:08.\n","\n","  Average training loss: 0.14\n","  Training epoch took: 0:01:39\n","\n","Running Validation...\n","  Accuracy: 0.96\n","  Validation took: 0:00:07\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:01:08.\n","\n","  Average training loss: 0.07\n","  Training epoch took: 0:01:39\n","\n","Running Validation...\n","  Accuracy: 0.96\n","  Validation took: 0:00:07\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:01:08.\n","\n","  Average training loss: 0.04\n","  Training epoch took: 0:01:39\n","\n","Running Validation...\n","  Accuracy: 0.96\n","  Validation took: 0:00:07\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","source":["7 min training time"],"metadata":{"id":"4d2MZ8YtqmvO"}},{"cell_type":"code","source":["print(loss_values) #Having a view of stored loss values in the list"],"metadata":{"id":"btUsZ5vMyjwt","outputId":"800a23a4-254f-4f48-d39d-25da165cf013","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198855431,"user_tz":-120,"elapsed":422,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3337289678974677, 0.13749465501030622, 0.07031703320473938, 0.03965484361180057]\n"]}]},{"cell_type":"code","source":["#Loading the test data and applying the same preprocessing techniques which we performed on the train data\n","import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/test_set_verif.csv')\n","\n","# Report the number of sentences.\n","print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Create sentence and label lists\n","sentences = df.sentence.values\n","labels = df.personal.values\n","\n","labels = le.transform(labels)\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                   )\n","    \n","    input_ids.append(encoded_sent)\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","# Convert to tensors.\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"mAN0LZBOOPVh","outputId":"61d7c9a3-acde-4091-9202-03580393a4dc","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198861569,"user_tz":-120,"elapsed":2982,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of test sentences: 638\n","\n"]}]},{"cell_type":"code","source":["#Evaluating our model on the test set\n","\n","# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n"],"metadata":{"id":"Hba10sXR7Xi6","outputId":"de56ba76-52b4-48bc-de50-058afe1b9900","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198889093,"user_tz":-120,"elapsed":13560,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 638 test sentences...\n"]}]},{"cell_type":"markdown","source":["We will use Matthews Correlation Coefficient(MCC) to evaluate our model. \n","MCC is used in many areas of Natural Language Processing. Also, it's a great metric to be used for imbalanced dataset\n","\n","Link: https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a\n"],"metadata":{"id":"-5jscIM8R4Gv"}},{"cell_type":"code","source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"],"metadata":{"id":"cRaZQ4XC7kLs","outputId":"f5218640-66fe-4a21-c500-ed35296cc75a","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198892838,"user_tz":-120,"elapsed":397,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating Matthews Corr. Coef. for each batch...\n"]}]},{"cell_type":"code","source":["# Combine the predictions for each batch into a single list of 0s and 1s.\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = [item for sublist in true_labels for item in sublist]\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('MCC: %.3f' % mcc)"],"metadata":{"id":"oCYZa1lQ8Jn8","outputId":"528ffe49-d685-48c5-b8c5-31809ec2eb64","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657198895566,"user_tz":-120,"elapsed":388,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["MCC: 0.827\n"]}]},{"cell_type":"code","source":["#for i in range(len(flat_true_labels)):\n","#  print(flat_true_labels[i], flat_predictions[i])\n","print(len(flat_predictions))\n","unique, counts = np.unique(flat_predictions, return_counts=True)\n","#unique, counts = np.unique(flat_true_labels, return_counts=True)\n","dict(zip(unique, counts))\n","\n","# 0 = NonArg\n","# 1 = UnVerif\n","# 2 = Verif"],"metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"aNo9EERdbNgM","executionInfo":{"status":"ok","timestamp":1657198899654,"user_tz":-120,"elapsed":392,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"1c7c986d-68e3-4d23-f3ae-04da47fd0651"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["638\n"]},{"output_type":"execute_result","data":{"text/plain":["{0: 517, 1: 121}"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["def show_plot(cm, labels):\n","        ax= plt.subplot()\n","        sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n","\n","        # labels, title and ticks\n","        ax.set_xlabel('Predicted Labels');ax.set_ylabel('True Labels'); \n","        ax.set_title('Confusion Matrix'); \n","        ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n","        plt.show()"],"metadata":{"id":"cqkYrvmxdzQK","executionInfo":{"status":"ok","timestamp":1657198902548,"user_tz":-120,"elapsed":411,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","accuracy = accuracy_score(flat_true_labels, flat_predictions)\n","print(\"Accuracy: {}\".format(accuracy))\n","\n","macro = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='macro')\n","micro = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","weighted = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='weighted')\n","\n","print(\"micro: precision = {} \\t recall = {} \\t f1 = {}\".format(micro[0], micro[1], micro[2]))\n","print(\"macro: precision = {} \\t recall = {} \\t f1 = {}\".format(macro[0], macro[1], macro[2]))\n","print(\"weighted: precision = {} \\t recall = {} \\t f1 = {}\".format(weighted[0], weighted[1], weighted[2]))\n","\n","per_class = precision_recall_fscore_support(flat_true_labels, flat_predictions, average=None, labels=[0, 1, 2])\n","print(\"Per Class:\")\n","print(\"NonArg: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][0], per_class[1][0], per_class[2][0]))\n","print(\"UnVerif: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][1], per_class[1][1], per_class[2][1]))\n","print(\"Verif: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][2], per_class[1][2], per_class[2][2]))\n","\n","cm = confusion_matrix(flat_true_labels, flat_predictions)\n","show_plot(cm, ['NonArg', 'UnVerif', 'Verif'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"id":"4oDkwP-Md16R","executionInfo":{"status":"ok","timestamp":1657198917381,"user_tz":-120,"elapsed":452,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"479102ef-907f-409d-a9a7-6c10db120b21"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9467084639498433\n","micro: precision = 0.9467084639498433 \t recall = 0.9467084639498433 \t f1 = 0.9467084639498433\n","macro: precision = 0.9133110603129946 \t recall = 0.9133110603129946 \t f1 = 0.9133110603129946\n","weighted: precision = 0.9467084639498433 \t recall = 0.9467084639498433 \t f1 = 0.9467084639498433\n","Per Class:\n","NonArg: precision = 0.9671179883945842 \t recall = 0.9671179883945842 \t f1 = 0.9671179883945842\n","UnVerif: precision = 0.859504132231405 \t recall = 0.859504132231405 \t f1 = 0.859504132231405\n","Verif: precision = 0.0 \t recall = 0.0 \t f1 = 0.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}