{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"DistilBert_Class.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"e5d80c96c4564e53b3c956fd8426844a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9f4638d146f46c68621e804d5ea55b1","IPY_MODEL_48eddb50128f4d679aefa0ed2c794432","IPY_MODEL_bc8bb4f05f34420787a87a9681989a2d"],"layout":"IPY_MODEL_43dbfa91662940b497255321c1bd1649"}},"f9f4638d146f46c68621e804d5ea55b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a97447d2b4584869b4ca693083811008","placeholder":"​","style":"IPY_MODEL_fb1eafaaaf724d76b952b8699720a301","value":"Downloading: 100%"}},"48eddb50128f4d679aefa0ed2c794432":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa49742ea2ec4b669cffa7d188af4636","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3cd46de254b44df85e5451b7e98f0d0","value":231508}},"bc8bb4f05f34420787a87a9681989a2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8183d15eca384f59a5822ac14237b89d","placeholder":"​","style":"IPY_MODEL_87682a3b10c144f3911bd5328294ce6f","value":" 226k/226k [00:00&lt;00:00, 1.56MB/s]"}},"43dbfa91662940b497255321c1bd1649":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a97447d2b4584869b4ca693083811008":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb1eafaaaf724d76b952b8699720a301":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa49742ea2ec4b669cffa7d188af4636":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3cd46de254b44df85e5451b7e98f0d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8183d15eca384f59a5822ac14237b89d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87682a3b10c144f3911bd5328294ce6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09aa9c8d03cc4e59894007af7365d639":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a2379a43b5d41b485e40ba37e76a12d","IPY_MODEL_dfe14b0a20db4ab5bab06b45198f5cdb","IPY_MODEL_91bce535823d4631a58694f87d0179ab"],"layout":"IPY_MODEL_eedd1c687ce14c8a97f853f8d3a8930e"}},"3a2379a43b5d41b485e40ba37e76a12d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b2f0568061c46d0a109b5de81099c2b","placeholder":"​","style":"IPY_MODEL_da8d6d99d15f411c9e7f3bd5aee5f9e9","value":"Downloading: 100%"}},"dfe14b0a20db4ab5bab06b45198f5cdb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_28e00391e9ee4b5f8a27dff541159a51","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2e198dff34174c58b4f0ec8d49b582f6","value":28}},"91bce535823d4631a58694f87d0179ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74506e3b5c2040c382378d5d582ed7f3","placeholder":"​","style":"IPY_MODEL_84deed4aa8334c189b03bf9da11673b3","value":" 28.0/28.0 [00:00&lt;00:00, 739B/s]"}},"eedd1c687ce14c8a97f853f8d3a8930e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b2f0568061c46d0a109b5de81099c2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da8d6d99d15f411c9e7f3bd5aee5f9e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28e00391e9ee4b5f8a27dff541159a51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e198dff34174c58b4f0ec8d49b582f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74506e3b5c2040c382378d5d582ed7f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84deed4aa8334c189b03bf9da11673b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40ac6d4014044e8d907dc60a9800a18c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ebe2d9d73ef4083b0e621b1de577dc7","IPY_MODEL_6dfe67ad443d4cc59210d1dd63e6e35b","IPY_MODEL_3a8ef894bdd04b1982ab6ab14cb5fc43"],"layout":"IPY_MODEL_8f752f99100348709601e9e24b6d3b08"}},"4ebe2d9d73ef4083b0e621b1de577dc7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c26bb4ffb7034686809eb190c5f64943","placeholder":"​","style":"IPY_MODEL_2f454f883e354734bf4efdd87b37c37b","value":"Downloading: 100%"}},"6dfe67ad443d4cc59210d1dd63e6e35b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_33fd3c5206ac43b497def8f63ca27265","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a0bb46ef7b847ae941231c69bc5f54d","value":483}},"3a8ef894bdd04b1982ab6ab14cb5fc43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bfbbc9cd2d348c08d49d134245e71d6","placeholder":"​","style":"IPY_MODEL_096d0ed3dd8641a786d5b4a11c8e1802","value":" 483/483 [00:00&lt;00:00, 14.9kB/s]"}},"8f752f99100348709601e9e24b6d3b08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c26bb4ffb7034686809eb190c5f64943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f454f883e354734bf4efdd87b37c37b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33fd3c5206ac43b497def8f63ca27265":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a0bb46ef7b847ae941231c69bc5f54d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7bfbbc9cd2d348c08d49d134245e71d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"096d0ed3dd8641a786d5b4a11c8e1802":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3121ba8b039f4e2090bdc5a16f09d1f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9773a0e141124d59a1742b6ccfc3e2d9","IPY_MODEL_43f47acdbb904c2a80cc304dc077d508","IPY_MODEL_e567b94bef6b4a0697a15b9875856875"],"layout":"IPY_MODEL_18073330ad6e4c8598ddd83c1d947a1f"}},"9773a0e141124d59a1742b6ccfc3e2d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_656fc6fb4e6d414a95da69acde10c3b0","placeholder":"​","style":"IPY_MODEL_242aaa7056614381be629004982df9df","value":"Downloading: 100%"}},"43f47acdbb904c2a80cc304dc077d508":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3c34ef9a9ae476dacce7ecf4f58a810","max":267967963,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dcc2598b349e48c1a6b80c06a9423358","value":267967963}},"e567b94bef6b4a0697a15b9875856875":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7252893ace743fc8921a3e46d571c43","placeholder":"​","style":"IPY_MODEL_6849b9d65c2048e2851ac4c638664a4b","value":" 256M/256M [00:04&lt;00:00, 51.1MB/s]"}},"18073330ad6e4c8598ddd83c1d947a1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"656fc6fb4e6d414a95da69acde10c3b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"242aaa7056614381be629004982df9df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3c34ef9a9ae476dacce7ecf4f58a810":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcc2598b349e48c1a6b80c06a9423358":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7252893ace743fc8921a3e46d571c43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6849b9d65c2048e2851ac4c638664a4b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["#Sentence Classification using BERT"],"metadata":{"id":"EKOTlwcmxmej"}},{"cell_type":"code","source":["import tensorflow as tf\n","# Verifying GPU availability (you have to turn it on in google colab)\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"metadata":{"id":"DEfSbAA4QHas","outputId":"9d7ef1eb-6e98-4e5a-ef52-927562b133ee","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657199443517,"user_tz":-120,"elapsed":8826,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\")"],"metadata":{"id":"oYsV4H8fCpZ-","trusted":true,"executionInfo":{"status":"ok","timestamp":1657199449413,"user_tz":-120,"elapsed":3530,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"0NmMdkZO8R6q","outputId":"0033f6e7-d633-4724-e8ed-9ae870aced50","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657199461457,"user_tz":-120,"elapsed":10733,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 50.7 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 16.3 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_UkeC7SG2krJ","outputId":"a3856d7e-e248-401e-8bbb-595e70352026","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657199479595,"user_tz":-120,"elapsed":15657,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Loading csv file as df and removing irrelevant / empty (= multiple frames) frames\n","train_df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/train_set.csv')\n","validation_df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/val_set.csv')"],"metadata":{"id":"wj2xpqX7exO1","executionInfo":{"status":"ok","timestamp":1657199482424,"user_tz":-120,"elapsed":1068,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(train_df.shape)\n","print(validation_df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hgs15asBe_Fr","executionInfo":{"status":"ok","timestamp":1657199483684,"user_tz":-120,"elapsed":161,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"016cd1bc-fa30-4f12-f5b7-d5ba412fbc52"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(7041, 13)\n","(1123, 13)\n"]}]},{"cell_type":"code","source":["from sklearn import preprocessing\n","\n","# Creating list of comments and corresponding labels\n","sentences_train = train_df.sentence.values\n","sentences_val = validation_df.sentence.values\n","\n","# Getting labels           \n","labels_train = train_df.verif.values\n","labels_val = validation_df.verif.values\n","\n","# Getting unique labels \n","un_labels = set()\n","for label in labels_train:\n","  un_labels.add(label)\n","print(len(un_labels), un_labels)\n","# Rewriting labels from str to int so it can be 'understood' by the classifier \n","le = preprocessing.LabelEncoder()\n","le.fit(sorted(un_labels))\n","\n","train_labels = le.transform(labels_train)\n","validation_labels = le.transform(labels_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9Bzi-owfIcJ","executionInfo":{"status":"ok","timestamp":1657199486553,"user_tz":-120,"elapsed":261,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"47311082-a140-45d3-c025-989774486915"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["3 {'NonArg', 'Verif', 'UnVerif'}\n"]}]},{"cell_type":"code","source":["from transformers import DistilBertTokenizer\n","\n","# Load the BERT tokenizer.\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"],"metadata":{"id":"Z474sSC6oe7A","trusted":true,"executionInfo":{"status":"ok","timestamp":1657199492525,"user_tz":-120,"elapsed":1797,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["e5d80c96c4564e53b3c956fd8426844a","f9f4638d146f46c68621e804d5ea55b1","48eddb50128f4d679aefa0ed2c794432","bc8bb4f05f34420787a87a9681989a2d","43dbfa91662940b497255321c1bd1649","a97447d2b4584869b4ca693083811008","fb1eafaaaf724d76b952b8699720a301","aa49742ea2ec4b669cffa7d188af4636","e3cd46de254b44df85e5451b7e98f0d0","8183d15eca384f59a5822ac14237b89d","87682a3b10c144f3911bd5328294ce6f","09aa9c8d03cc4e59894007af7365d639","3a2379a43b5d41b485e40ba37e76a12d","dfe14b0a20db4ab5bab06b45198f5cdb","91bce535823d4631a58694f87d0179ab","eedd1c687ce14c8a97f853f8d3a8930e","0b2f0568061c46d0a109b5de81099c2b","da8d6d99d15f411c9e7f3bd5aee5f9e9","28e00391e9ee4b5f8a27dff541159a51","2e198dff34174c58b4f0ec8d49b582f6","74506e3b5c2040c382378d5d582ed7f3","84deed4aa8334c189b03bf9da11673b3","40ac6d4014044e8d907dc60a9800a18c","4ebe2d9d73ef4083b0e621b1de577dc7","6dfe67ad443d4cc59210d1dd63e6e35b","3a8ef894bdd04b1982ab6ab14cb5fc43","8f752f99100348709601e9e24b6d3b08","c26bb4ffb7034686809eb190c5f64943","2f454f883e354734bf4efdd87b37c37b","33fd3c5206ac43b497def8f63ca27265","8a0bb46ef7b847ae941231c69bc5f54d","7bfbbc9cd2d348c08d49d134245e71d6","096d0ed3dd8641a786d5b4a11c8e1802"]},"outputId":"439b22b4-26b0-479f-af4a-988249ae896e"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5d80c96c4564e53b3c956fd8426844a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09aa9c8d03cc4e59894007af7365d639"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40ac6d4014044e8d907dc60a9800a18c"}},"metadata":{}}]},{"cell_type":"code","source":["def sent_to_id(sentences):\n","  # Tokenize all of the sentences and map the tokens to thier word IDs.\n","  input_ids = []\n","\n","  # For every sentence...\n","  for sent in sentences:\n","      # `encode` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      encoded_sent = tokenizer.encode(\n","                          sent\n","                    )\n","      \n","      # Add the encoded sentence to the list.\n","      input_ids.append(encoded_sent)\n","\n","  # Print sentence 0, now as a list of IDs.\n","  print('Original: ', sentences[0])\n","  print('Token IDs:', input_ids[0])\n","  return(input_ids)"],"metadata":{"id":"2bBdb3pt8LuQ","trusted":true,"executionInfo":{"status":"ok","timestamp":1657199496640,"user_tz":-120,"elapsed":181,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["train_inputs = sent_to_id(sentences_train)\n","validation_inputs = sent_to_id(sentences_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Yj-NSQ8gGEX","executionInfo":{"status":"ok","timestamp":1657199502850,"user_tz":-120,"elapsed":4576,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"20738128-2c2c-4df5-ac11-afe644fd4336"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  I have so many responses to this, but most of them stem from emotions, so I'm going to put them aside, and just go with two.\n","Token IDs: [101, 1045, 2031, 2061, 2116, 10960, 2000, 2023, 1010, 2021, 2087, 1997, 2068, 7872, 2013, 6699, 1010, 2061, 1045, 1005, 1049, 2183, 2000, 2404, 2068, 4998, 1010, 1998, 2074, 2175, 2007, 2048, 1012, 102]\n","Original:  You don't make any attempt to cite your claims.\n","Token IDs: [101, 2017, 2123, 1005, 1056, 2191, 2151, 3535, 2000, 21893, 2115, 4447, 1012, 102]\n"]}]},{"cell_type":"code","source":["print('Max sentence length(train): ', max([len(sen) for sen in train_inputs]))\n","print('Max sentence length(validation): ', max([len(sen) for sen in validation_inputs]))"],"metadata":{"id":"JhUZO9vc_l6T","outputId":"7ddb3f7f-5551-4abc-b590-e09bcb0b079b","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657199505625,"user_tz":-120,"elapsed":183,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sentence length(train):  203\n","Max sentence length(validation):  137\n"]}]},{"cell_type":"code","source":["# We will use some utility function from tensorflow(Tensorflow was my first crush)\n","from keras.preprocessing.sequence import pad_sequences\n","\n","MAX_LEN = 305\n","\n","#Padding the input to the max length that is 64\n","train_inputs = pad_sequences(train_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","validation_inputs = pad_sequences(validation_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")"],"metadata":{"id":"Cp9BPRd1tMIo","trusted":true,"executionInfo":{"status":"ok","timestamp":1657199508748,"user_tz":-120,"elapsed":197,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def attentionmasks(input_ids):\n","  # Creating the attention masks\n","  attention_masks = []\n","\n","  # For each sentence...\n","  for sent in input_ids:\n","      \n","      # Create the attention mask.\n","      #   - If a token ID is 0, then it's padding, set the mask to 0.\n","      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","      att_mask = [int(token_id > 0) for token_id in sent]\n","      \n","      # Store the attention mask for this sentence.\n","      attention_masks.append(att_mask)\n","  return(attention_masks)"],"metadata":{"id":"cDoC24LeEv3N","trusted":true,"executionInfo":{"status":"ok","timestamp":1657199511309,"user_tz":-120,"elapsed":2,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["train_masks = attentionmasks(train_inputs)\n","validation_masks = attentionmasks(validation_inputs)"],"metadata":{"id":"-l_lHhwGiSiX","executionInfo":{"status":"ok","timestamp":1657199513536,"user_tz":-120,"elapsed":1109,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#Converting the input data to the tensor , which can be feeded to the model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"metadata":{"id":"jw5K2A5Ko1RF","trusted":true,"executionInfo":{"status":"ok","timestamp":1657199515159,"user_tz":-120,"elapsed":257,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#Creating the DataLoader which will help us to load data into the GPU/CPU\n","batch_size = 32\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"],"metadata":{"id":"GEgLpFVlo1Z-","trusted":true,"executionInfo":{"status":"ok","timestamp":1657199518171,"user_tz":-120,"elapsed":253,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["#Loading the pre-trained BERT model from huggingface library\n","\n","from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = DistilBertForSequenceClassification.from_pretrained(\n","    \"distilbert-base-uncased\", \n","    num_labels = 3,   \n","    output_attentions = False, \n","    output_hidden_states = False, )\n","\n","# Teeling the model to run on GPU\n","model.cuda()"],"metadata":{"id":"gFsCTp_mporB","outputId":"03643544-4fb3-4fc5-f4f6-c7c2b9b483ea","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3121ba8b039f4e2090bdc5a16f09d1f9","9773a0e141124d59a1742b6ccfc3e2d9","43f47acdbb904c2a80cc304dc077d508","e567b94bef6b4a0697a15b9875856875","18073330ad6e4c8598ddd83c1d947a1f","656fc6fb4e6d414a95da69acde10c3b0","242aaa7056614381be629004982df9df","a3c34ef9a9ae476dacce7ecf4f58a810","dcc2598b349e48c1a6b80c06a9423358","d7252893ace743fc8921a3e46d571c43","6849b9d65c2048e2851ac4c638664a4b"]},"executionInfo":{"status":"ok","timestamp":1657199556159,"user_tz":-120,"elapsed":16548,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3121ba8b039f4e2090bdc5a16f09d1f9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# AdamW is an optimizer which is a Adam Optimzier with weight-decay-fix\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, \n","                  eps = 1e-8 \n","                )\n"],"metadata":{"id":"GLs72DuMODJO","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657199559595,"user_tz":-120,"elapsed":275,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"cdfe67e1-29a6-4383-adcc-a98ac23a82a0"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","scheduler"],"metadata":{"id":"-p0upAhhRiIx","outputId":"bd0b9570-45c2-4fed-b274-82bb1efeaacf","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657199562686,"user_tz":-120,"elapsed":176,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.optim.lr_scheduler.LambdaLR at 0x7f359c5e5410>"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["Define a helper function for calculating accuracy."],"metadata":{"id":"pE5B99H5H2-W"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"9cQNvaZ9bnyy","trusted":true,"executionInfo":{"status":"ok","timestamp":1657199565038,"user_tz":-120,"elapsed":150,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["#Creating the helper function to have a watch on elapsed time\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"metadata":{"id":"gpt6tR83keZD","trusted":true,"executionInfo":{"status":"ok","timestamp":1657199567248,"user_tz":-120,"elapsed":1,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#Let's start the training process\n","\n","import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids,  \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"metadata":{"id":"6J-FYdx6nFE_","outputId":"157af75c-6b0c-46f5-de91-36a64a0dbd4c","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657200691165,"user_tz":-120,"elapsed":794416,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    221.    Elapsed: 0:00:31.\n","  Batch    80  of    221.    Elapsed: 0:01:03.\n","  Batch   120  of    221.    Elapsed: 0:01:36.\n","  Batch   160  of    221.    Elapsed: 0:02:09.\n","  Batch   200  of    221.    Elapsed: 0:02:44.\n","\n","  Average training loss: 0.65\n","  Training epoch took: 0:03:01\n","\n","Running Validation...\n","  Accuracy: 0.77\n","  Validation took: 0:00:12\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    221.    Elapsed: 0:00:34.\n","  Batch    80  of    221.    Elapsed: 0:01:08.\n","  Batch   120  of    221.    Elapsed: 0:01:43.\n","  Batch   160  of    221.    Elapsed: 0:02:17.\n","  Batch   200  of    221.    Elapsed: 0:02:51.\n","\n","  Average training loss: 0.47\n","  Training epoch took: 0:03:08\n","\n","Running Validation...\n","  Accuracy: 0.78\n","  Validation took: 0:00:12\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    221.    Elapsed: 0:00:34.\n","  Batch    80  of    221.    Elapsed: 0:01:08.\n","  Batch   120  of    221.    Elapsed: 0:01:43.\n","  Batch   160  of    221.    Elapsed: 0:02:17.\n","  Batch   200  of    221.    Elapsed: 0:02:51.\n","\n","  Average training loss: 0.36\n","  Training epoch took: 0:03:08\n","\n","Running Validation...\n","  Accuracy: 0.77\n","  Validation took: 0:00:12\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    221.    Elapsed: 0:00:34.\n","  Batch    80  of    221.    Elapsed: 0:01:08.\n","  Batch   120  of    221.    Elapsed: 0:01:42.\n","  Batch   160  of    221.    Elapsed: 0:02:17.\n","  Batch   200  of    221.    Elapsed: 0:02:51.\n","\n","  Average training loss: 0.30\n","  Training epoch took: 0:03:08\n","\n","Running Validation...\n","  Accuracy: 0.77\n","  Validation took: 0:00:12\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","source":["13 min training time"],"metadata":{"id":"4d2MZ8YtqmvO"}},{"cell_type":"code","source":["print(loss_values) #Having a view of stored loss values in the list"],"metadata":{"id":"btUsZ5vMyjwt","outputId":"de3407d2-add5-4222-fe01-f2e33b010f2a","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657200753266,"user_tz":-120,"elapsed":185,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.6516277164355662, 0.46605780788136825, 0.3605600854390347, 0.29613548176725524]\n"]}]},{"cell_type":"code","source":["#Loading the test data and applying the same preprocessing techniques which we performed on the train data\n","import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/test_set.csv')\n","\n","# Report the number of sentences.\n","print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Create sentence and label lists\n","sentences = df.sentence.values\n","labels = df.verif.values\n","\n","labels = le.transform(labels)\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                   )\n","    \n","    input_ids.append(encoded_sent)\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","# Convert to tensors.\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"mAN0LZBOOPVh","outputId":"4c959c05-f7ad-4dce-b4cd-fa5fa0094249","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657200757015,"user_tz":-120,"elapsed":2038,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of test sentences: 1,962\n","\n"]}]},{"cell_type":"code","source":["#Evaluating our model on the test set\n","\n","# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n"],"metadata":{"id":"Hba10sXR7Xi6","outputId":"ea8afe6e-6c18-4502-8056-e3f0ed31c501","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657200785452,"user_tz":-120,"elapsed":21441,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 1,962 test sentences...\n"]}]},{"cell_type":"markdown","source":["We will use Matthews Correlation Coefficient(MCC) to evaluate our model. \n","MCC is used in many areas of Natural Language Processing. Also, it's a great metric to be used for imbalanced dataset\n","\n","Link: https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a\n"],"metadata":{"id":"-5jscIM8R4Gv"}},{"cell_type":"code","source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"],"metadata":{"id":"cRaZQ4XC7kLs","outputId":"d227e524-1287-4461-90d6-acb3895e205e","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657200788747,"user_tz":-120,"elapsed":184,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating Matthews Corr. Coef. for each batch...\n"]}]},{"cell_type":"code","source":["# Combine the predictions for each batch into a single list of 0s and 1s.\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = [item for sublist in true_labels for item in sublist]\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('MCC: %.3f' % mcc)"],"metadata":{"id":"oCYZa1lQ8Jn8","outputId":"52f05c4a-595f-4ddb-de47-7457ba1387b4","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657200792507,"user_tz":-120,"elapsed":180,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["MCC: 0.559\n"]}]},{"cell_type":"code","source":["#for i in range(len(flat_true_labels)):\n","#  print(flat_true_labels[i], flat_predictions[i])\n","print(len(flat_predictions))\n","unique, counts = np.unique(flat_predictions, return_counts=True)\n","#unique, counts = np.unique(flat_true_labels, return_counts=True)\n","dict(zip(unique, counts))\n","\n","# 0 = NonArg\n","# 1 = UnVerif\n","# 2 = Verif"],"metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"aNo9EERdbNgM","executionInfo":{"status":"ok","timestamp":1657200794801,"user_tz":-120,"elapsed":182,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"e18feaa4-f6ab-4433-deb1-1f28a6bd4ca5"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["1962\n"]},{"output_type":"execute_result","data":{"text/plain":["{0: 237, 1: 1242, 2: 483}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["def show_plot(cm, labels):\n","        ax= plt.subplot()\n","        sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n","\n","        # labels, title and ticks\n","        ax.set_xlabel('Predicted Labels');ax.set_ylabel('True Labels'); \n","        ax.set_title('Confusion Matrix'); \n","        ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n","        plt.show()"],"metadata":{"id":"cqkYrvmxdzQK","executionInfo":{"status":"ok","timestamp":1657200796768,"user_tz":-120,"elapsed":160,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","accuracy = accuracy_score(flat_true_labels, flat_predictions)\n","print(\"Accuracy: {}\".format(accuracy))\n","\n","macro = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='macro')\n","micro = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","weighted = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='weighted')\n","\n","print(\"micro: precision = {} \\t recall = {} \\t f1 = {}\".format(micro[0], micro[1], micro[2]))\n","print(\"macro: precision = {} \\t recall = {} \\t f1 = {}\".format(macro[0], macro[1], macro[2]))\n","print(\"weighted: precision = {} \\t recall = {} \\t f1 = {}\".format(weighted[0], weighted[1], weighted[2]))\n","\n","per_class = precision_recall_fscore_support(flat_true_labels, flat_predictions, average=None, labels=[0, 1, 2])\n","print(\"Per Class:\")\n","print(\"NonArg: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][0], per_class[1][0], per_class[2][0]))\n","print(\"UnVerif: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][1], per_class[1][1], per_class[2][1]))\n","print(\"Verif: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][2], per_class[1][2], per_class[2][2]))\n","\n","cm = confusion_matrix(flat_true_labels, flat_predictions)\n","show_plot(cm, ['NonArg', 'UnVerif', 'Verif'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"id":"4oDkwP-Md16R","executionInfo":{"status":"ok","timestamp":1657200798684,"user_tz":-120,"elapsed":668,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"8a9d0af3-0325-4df8-e300-3040d2d65019"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7446483180428135\n","micro: precision = 0.7446483180428135 \t recall = 0.7446483180428135 \t f1 = 0.7446483180428135\n","macro: precision = 0.7790134133924265 \t recall = 0.7191707093439255 \t f1 = 0.7408670098831408\n","weighted: precision = 0.7480401425692165 \t recall = 0.7446483180428135 \t f1 = 0.7382106052914297\n","Per Class:\n","NonArg: precision = 0.890295358649789 \t recall = 0.7455830388692579 \t f1 = 0.8115384615384615\n","UnVerif: precision = 0.7262479871175523 \t recall = 0.866474543707973 \t f1 = 0.7901883486640384\n","Verif: precision = 0.7204968944099379 \t recall = 0.5454545454545454 \t f1 = 0.6208742194469223\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}