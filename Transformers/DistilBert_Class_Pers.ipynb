{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"DistilBert_Class_Pers.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"5595e31d7e174936afdb740a54daf3fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c275e9a4f831423395a04ce38d5dfed9","IPY_MODEL_64434d04fb9a4e28992da71aa36a3996","IPY_MODEL_a34b84317e1b47b9817a470f5076224e"],"layout":"IPY_MODEL_3f76c293104f40ca8fe2472db0466a46"}},"c275e9a4f831423395a04ce38d5dfed9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44f6043ebc3b49828b31720bd512efc8","placeholder":"​","style":"IPY_MODEL_a76c935788014992baf8849b964948ff","value":"Downloading: 100%"}},"64434d04fb9a4e28992da71aa36a3996":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a0fff274da543538b16636e4362199d","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cdc1bc556a314de9a3ad00058a725a27","value":231508}},"a34b84317e1b47b9817a470f5076224e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40e22018c76247ab8850e694d0466ece","placeholder":"​","style":"IPY_MODEL_0524ed9602b34b52a2b6337fb8cfc32c","value":" 226k/226k [00:00&lt;00:00, 713kB/s]"}},"3f76c293104f40ca8fe2472db0466a46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44f6043ebc3b49828b31720bd512efc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a76c935788014992baf8849b964948ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a0fff274da543538b16636e4362199d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdc1bc556a314de9a3ad00058a725a27":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40e22018c76247ab8850e694d0466ece":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0524ed9602b34b52a2b6337fb8cfc32c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f7bf5b45a6f46e8a02b3a773a75243b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_76f03cb82e2548dfb7615e996721eb16","IPY_MODEL_7264b0ab45f0416890cd353274e51086","IPY_MODEL_8c34b3adfed14beb8da75db36349d613"],"layout":"IPY_MODEL_570721d4b64049889dc04e8d19828fa6"}},"76f03cb82e2548dfb7615e996721eb16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa5c1f78c00b4bd4b1cefdec9dd0394a","placeholder":"​","style":"IPY_MODEL_d51dfe61fa724b1b934ebc06d179e62d","value":"Downloading: 100%"}},"7264b0ab45f0416890cd353274e51086":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_045d89016d8b4aebbb39b28d5b46b9c9","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3baa708cb36e4e66a8e076842559e9ac","value":28}},"8c34b3adfed14beb8da75db36349d613":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6c090288b54428d9d4989f432609195","placeholder":"​","style":"IPY_MODEL_0b806d76f4b44c2583d4f142827b562f","value":" 28.0/28.0 [00:00&lt;00:00, 718B/s]"}},"570721d4b64049889dc04e8d19828fa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa5c1f78c00b4bd4b1cefdec9dd0394a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d51dfe61fa724b1b934ebc06d179e62d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"045d89016d8b4aebbb39b28d5b46b9c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3baa708cb36e4e66a8e076842559e9ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6c090288b54428d9d4989f432609195":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b806d76f4b44c2583d4f142827b562f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c68cc11b0d7481b85e02806b60a306d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2699ee4a03754dff8d17b4fbd5975ebe","IPY_MODEL_88a10e5364c5466d80c8f0d9062f2c4f","IPY_MODEL_a52825a28f93435dbd9e096e537bbdbe"],"layout":"IPY_MODEL_dfdaf59dc1b74193bed5368b20e47f71"}},"2699ee4a03754dff8d17b4fbd5975ebe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc50863cc25949d2bede79384eb85244","placeholder":"​","style":"IPY_MODEL_1b47e9ec962d46a0af4c000836e163a4","value":"Downloading: 100%"}},"88a10e5364c5466d80c8f0d9062f2c4f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_11ef85dfd4974d42965c6053fa307ba6","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_38eb7255e2e141dfb0dd17e8be8eeb02","value":483}},"a52825a28f93435dbd9e096e537bbdbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7696e329ed844199c1135ccca7ba1bf","placeholder":"​","style":"IPY_MODEL_62f03c21684e492dba58a231b522254a","value":" 483/483 [00:00&lt;00:00, 12.6kB/s]"}},"dfdaf59dc1b74193bed5368b20e47f71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc50863cc25949d2bede79384eb85244":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b47e9ec962d46a0af4c000836e163a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11ef85dfd4974d42965c6053fa307ba6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38eb7255e2e141dfb0dd17e8be8eeb02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a7696e329ed844199c1135ccca7ba1bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62f03c21684e492dba58a231b522254a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a076f53dd33441081b4ed090d4e8b47":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98cf33afe12746c2ae62729a14a477b7","IPY_MODEL_7c5e1cd6c02c48799e50a8b2963742ae","IPY_MODEL_cd29f72af48e454892319d5f5ce4dcb9"],"layout":"IPY_MODEL_892f6683f85b49579e46d3957ece80ca"}},"98cf33afe12746c2ae62729a14a477b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5c52d9aafa6444b8e68686a2e8a1cc9","placeholder":"​","style":"IPY_MODEL_858b23293eba49f6908305a60d4ee9bc","value":"Downloading: 100%"}},"7c5e1cd6c02c48799e50a8b2963742ae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ee085111969428fae7077d246d51dde","max":267967963,"min":0,"orientation":"horizontal","style":"IPY_MODEL_31513a959c3147dfa4df345bf9b86689","value":267967963}},"cd29f72af48e454892319d5f5ce4dcb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9df79d84997046119074f2a545f328f0","placeholder":"​","style":"IPY_MODEL_c45011a08866411890b363ce4c81b464","value":" 256M/256M [00:04&lt;00:00, 61.8MB/s]"}},"892f6683f85b49579e46d3957ece80ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5c52d9aafa6444b8e68686a2e8a1cc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"858b23293eba49f6908305a60d4ee9bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ee085111969428fae7077d246d51dde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31513a959c3147dfa4df345bf9b86689":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9df79d84997046119074f2a545f328f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c45011a08866411890b363ce4c81b464":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["#Sentence Classification using BERT"],"metadata":{"id":"EKOTlwcmxmej"}},{"cell_type":"code","source":["import tensorflow as tf\n","# Verifying GPU availability (you have to turn it on in google colab)\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"metadata":{"id":"DEfSbAA4QHas","outputId":"cc67289d-4558-48a1-eb9a-67ee44a7609e","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657200931643,"user_tz":-120,"elapsed":8587,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\")"],"metadata":{"id":"oYsV4H8fCpZ-","trusted":true,"executionInfo":{"status":"ok","timestamp":1657200936621,"user_tz":-120,"elapsed":4035,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"0NmMdkZO8R6q","outputId":"4c58c4e7-3b4e-48ca-91d2-2cbf9f859a57","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657200948435,"user_tz":-120,"elapsed":10279,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 6.3 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 12.8 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 72.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 48.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_UkeC7SG2krJ","outputId":"b1a3d79f-89cf-486b-d114-2e252b0cef63","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657200966615,"user_tz":-120,"elapsed":16260,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Loading csv file as df and removing irrelevant / empty (= multiple frames) frames\n","train_df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/train_set_verif.csv')\n","validation_df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/val_set_verif.csv')"],"metadata":{"id":"wj2xpqX7exO1","executionInfo":{"status":"ok","timestamp":1657200969006,"user_tz":-120,"elapsed":1438,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(train_df.shape)\n","print(validation_df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hgs15asBe_Fr","executionInfo":{"status":"ok","timestamp":1657200970232,"user_tz":-120,"elapsed":1,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"59da88f3-19b3-4460-ee27-bb1fa69c493c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(1870, 13)\n","(330, 13)\n"]}]},{"cell_type":"code","source":["from sklearn import preprocessing\n","\n","# Creating list of comments and corresponding labels\n","sentences_train = train_df.sentence.values\n","sentences_val = validation_df.sentence.values\n","\n","# Getting labels           \n","labels_train = train_df.personal.values\n","labels_val = validation_df.personal.values\n","\n","# Getting unique labels \n","un_labels = set()\n","for label in labels_train:\n","  un_labels.add(label)\n","print(len(un_labels), un_labels)\n","# Rewriting labels from str to int so it can be 'understood' by the classifier \n","le = preprocessing.LabelEncoder()\n","le.fit(sorted(un_labels))\n","\n","train_labels = le.transform(labels_train)\n","validation_labels = le.transform(labels_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9Bzi-owfIcJ","executionInfo":{"status":"ok","timestamp":1657200972479,"user_tz":-120,"elapsed":226,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"4f36d1cf-7655-4a06-afa5-f4bee41eb451"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2 {'Pers', 'NonPers'}\n"]}]},{"cell_type":"code","source":["from transformers import DistilBertTokenizer\n","\n","# Load the BERT tokenizer.\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"],"metadata":{"id":"Z474sSC6oe7A","trusted":true,"executionInfo":{"status":"ok","timestamp":1657200978252,"user_tz":-120,"elapsed":2976,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["5595e31d7e174936afdb740a54daf3fc","c275e9a4f831423395a04ce38d5dfed9","64434d04fb9a4e28992da71aa36a3996","a34b84317e1b47b9817a470f5076224e","3f76c293104f40ca8fe2472db0466a46","44f6043ebc3b49828b31720bd512efc8","a76c935788014992baf8849b964948ff","9a0fff274da543538b16636e4362199d","cdc1bc556a314de9a3ad00058a725a27","40e22018c76247ab8850e694d0466ece","0524ed9602b34b52a2b6337fb8cfc32c","3f7bf5b45a6f46e8a02b3a773a75243b","76f03cb82e2548dfb7615e996721eb16","7264b0ab45f0416890cd353274e51086","8c34b3adfed14beb8da75db36349d613","570721d4b64049889dc04e8d19828fa6","fa5c1f78c00b4bd4b1cefdec9dd0394a","d51dfe61fa724b1b934ebc06d179e62d","045d89016d8b4aebbb39b28d5b46b9c9","3baa708cb36e4e66a8e076842559e9ac","f6c090288b54428d9d4989f432609195","0b806d76f4b44c2583d4f142827b562f","3c68cc11b0d7481b85e02806b60a306d","2699ee4a03754dff8d17b4fbd5975ebe","88a10e5364c5466d80c8f0d9062f2c4f","a52825a28f93435dbd9e096e537bbdbe","dfdaf59dc1b74193bed5368b20e47f71","dc50863cc25949d2bede79384eb85244","1b47e9ec962d46a0af4c000836e163a4","11ef85dfd4974d42965c6053fa307ba6","38eb7255e2e141dfb0dd17e8be8eeb02","a7696e329ed844199c1135ccca7ba1bf","62f03c21684e492dba58a231b522254a"]},"outputId":"d55f8f87-00db-44d5-8bd4-bf9793507e90"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5595e31d7e174936afdb740a54daf3fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f7bf5b45a6f46e8a02b3a773a75243b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c68cc11b0d7481b85e02806b60a306d"}},"metadata":{}}]},{"cell_type":"code","source":["def sent_to_id(sentences):\n","  # Tokenize all of the sentences and map the tokens to thier word IDs.\n","  input_ids = []\n","\n","  # For every sentence...\n","  for sent in sentences:\n","      # `encode` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      encoded_sent = tokenizer.encode(\n","                          sent\n","                    )\n","      \n","      # Add the encoded sentence to the list.\n","      input_ids.append(encoded_sent)\n","\n","  # Print sentence 0, now as a list of IDs.\n","  print('Original: ', sentences[0])\n","  print('Token IDs:', input_ids[0])\n","  return(input_ids)"],"metadata":{"id":"2bBdb3pt8LuQ","trusted":true,"executionInfo":{"status":"ok","timestamp":1657200981886,"user_tz":-120,"elapsed":1,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["train_inputs = sent_to_id(sentences_train)\n","validation_inputs = sent_to_id(sentences_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Yj-NSQ8gGEX","executionInfo":{"status":"ok","timestamp":1657200986448,"user_tz":-120,"elapsed":1511,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"3f629c27-722b-4637-8253-ec3e0a999b83"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  Most of the people in America don't see a dime of social program money until they turn 65, but they start contributing to them at 16 (or whenever they get their first job).\n","Token IDs: [101, 2087, 1997, 1996, 2111, 1999, 2637, 2123, 1005, 1056, 2156, 1037, 27211, 1997, 2591, 2565, 2769, 2127, 2027, 2735, 3515, 1010, 2021, 2027, 2707, 8020, 2000, 2068, 2012, 2385, 1006, 2030, 7188, 2027, 2131, 2037, 2034, 3105, 1007, 1012, 102]\n","Original:  You don't make any attempt to cite your claims.\n","Token IDs: [101, 2017, 2123, 1005, 1056, 2191, 2151, 3535, 2000, 21893, 2115, 4447, 1012, 102]\n"]}]},{"cell_type":"code","source":["print('Max sentence length(train): ', max([len(sen) for sen in train_inputs]))\n","print('Max sentence length(validation): ', max([len(sen) for sen in validation_inputs]))"],"metadata":{"id":"JhUZO9vc_l6T","outputId":"88731435-63cc-489c-f23b-ac9124a45db1","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657200993504,"user_tz":-120,"elapsed":342,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sentence length(train):  122\n","Max sentence length(validation):  137\n"]}]},{"cell_type":"code","source":["# We will use some utility function from tensorflow(Tensorflow was my first crush)\n","from keras.preprocessing.sequence import pad_sequences\n","\n","MAX_LEN = 305\n","\n","#Padding the input to the max length that is 64\n","train_inputs = pad_sequences(train_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","validation_inputs = pad_sequences(validation_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")"],"metadata":{"id":"Cp9BPRd1tMIo","trusted":true,"executionInfo":{"status":"ok","timestamp":1657200995301,"user_tz":-120,"elapsed":211,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def attentionmasks(input_ids):\n","  # Creating the attention masks\n","  attention_masks = []\n","\n","  # For each sentence...\n","  for sent in input_ids:\n","      \n","      # Create the attention mask.\n","      #   - If a token ID is 0, then it's padding, set the mask to 0.\n","      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","      att_mask = [int(token_id > 0) for token_id in sent]\n","      \n","      # Store the attention mask for this sentence.\n","      attention_masks.append(att_mask)\n","  return(attention_masks)"],"metadata":{"id":"cDoC24LeEv3N","trusted":true,"executionInfo":{"status":"ok","timestamp":1657200996991,"user_tz":-120,"elapsed":224,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["train_masks = attentionmasks(train_inputs)\n","validation_masks = attentionmasks(validation_inputs)"],"metadata":{"id":"-l_lHhwGiSiX","executionInfo":{"status":"ok","timestamp":1657200999816,"user_tz":-120,"elapsed":522,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#Converting the input data to the tensor , which can be feeded to the model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"metadata":{"id":"jw5K2A5Ko1RF","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201001370,"user_tz":-120,"elapsed":217,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#Creating the DataLoader which will help us to load data into the GPU/CPU\n","batch_size = 32\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"],"metadata":{"id":"GEgLpFVlo1Z-","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201003730,"user_tz":-120,"elapsed":214,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["#Loading the pre-trained BERT model from huggingface library\n","\n","from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = DistilBertForSequenceClassification.from_pretrained(\n","    \"distilbert-base-uncased\", \n","    num_labels = 2,   \n","    output_attentions = False, \n","    output_hidden_states = False, )\n","\n","# Teeling the model to run on GPU\n","model.cuda()"],"metadata":{"id":"gFsCTp_mporB","outputId":"a1deda73-f980-4cd6-c494-6c56db2b9061","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4a076f53dd33441081b4ed090d4e8b47","98cf33afe12746c2ae62729a14a477b7","7c5e1cd6c02c48799e50a8b2963742ae","cd29f72af48e454892319d5f5ce4dcb9","892f6683f85b49579e46d3957ece80ca","f5c52d9aafa6444b8e68686a2e8a1cc9","858b23293eba49f6908305a60d4ee9bc","6ee085111969428fae7077d246d51dde","31513a959c3147dfa4df345bf9b86689","9df79d84997046119074f2a545f328f0","c45011a08866411890b363ce4c81b464"]},"executionInfo":{"status":"ok","timestamp":1657201027249,"user_tz":-120,"elapsed":17957,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a076f53dd33441081b4ed090d4e8b47"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["DistilBertForSequenceClassification(\n","  (distilbert): DistilBertModel(\n","    (embeddings): Embeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (transformer): Transformer(\n","      (layer): ModuleList(\n","        (0): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (1): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (2): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (3): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (4): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","        (5): TransformerBlock(\n","          (attention): MultiHeadSelfAttention(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n","            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (ffn): FFN(\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n","            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n","            (activation): GELUActivation()\n","          )\n","          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        )\n","      )\n","    )\n","  )\n","  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","  (dropout): Dropout(p=0.2, inplace=False)\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# AdamW is an optimizer which is a Adam Optimzier with weight-decay-fix\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, \n","                  eps = 1e-8 \n","                )\n"],"metadata":{"id":"GLs72DuMODJO","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201030506,"user_tz":-120,"elapsed":202,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"664dbcbb-8b8c-42ae-81c0-5f2b4551a9c9"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","scheduler"],"metadata":{"id":"-p0upAhhRiIx","outputId":"318a716d-bfc6-4339-e1af-7d5481c2e232","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201032333,"user_tz":-120,"elapsed":206,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.optim.lr_scheduler.LambdaLR at 0x7fb02163e550>"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["Define a helper function for calculating accuracy."],"metadata":{"id":"pE5B99H5H2-W"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"9cQNvaZ9bnyy","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201034287,"user_tz":-120,"elapsed":208,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["#Creating the helper function to have a watch on elapsed time\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"metadata":{"id":"gpt6tR83keZD","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201036267,"user_tz":-120,"elapsed":206,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#Let's start the training process\n","\n","import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"metadata":{"id":"6J-FYdx6nFE_","outputId":"51e0eb2b-c38c-48a6-a6a4-e822d4f280cc","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201249321,"user_tz":-120,"elapsed":208970,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:00:31.\n","\n","  Average training loss: 0.33\n","  Training epoch took: 0:00:46\n","\n","Running Validation...\n","  Accuracy: 0.93\n","  Validation took: 0:00:03\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:00:34.\n","\n","  Average training loss: 0.15\n","  Training epoch took: 0:00:50\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation took: 0:00:04\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:00:33.\n","\n","  Average training loss: 0.10\n","  Training epoch took: 0:00:49\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation took: 0:00:04\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:00:34.\n","\n","  Average training loss: 0.08\n","  Training epoch took: 0:00:50\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation took: 0:00:04\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","source":["3 min training time"],"metadata":{"id":"4d2MZ8YtqmvO"}},{"cell_type":"code","source":["print(loss_values) #Having a view of stored loss values in the list"],"metadata":{"id":"btUsZ5vMyjwt","outputId":"e0525e63-194f-483e-c553-9338d1ca43f2","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201374668,"user_tz":-120,"elapsed":243,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3331390146986913, 0.15312187774580413, 0.10369561161151376, 0.07815136267977246]\n"]}]},{"cell_type":"code","source":["#Loading the test data and applying the same preprocessing techniques which we performed on the train data\n","import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/test_set_verif.csv')\n","\n","# Report the number of sentences.\n","print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Create sentence and label lists\n","sentences = df.sentence.values\n","labels = df.personal.values\n","\n","labels = le.transform(labels)\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                   )\n","    \n","    input_ids.append(encoded_sent)\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","# Convert to tensors.\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"mAN0LZBOOPVh","outputId":"27b9241c-7685-41bb-ba3c-7da17851a960","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201377621,"user_tz":-120,"elapsed":1030,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of test sentences: 638\n","\n"]}]},{"cell_type":"code","source":["#Evaluating our model on the test set\n","\n","# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n"],"metadata":{"id":"Hba10sXR7Xi6","outputId":"c9b46d37-5341-4d10-a962-b205876ac991","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201388935,"user_tz":-120,"elapsed":6782,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 638 test sentences...\n"]}]},{"cell_type":"markdown","source":["We will use Matthews Correlation Coefficient(MCC) to evaluate our model. \n","MCC is used in many areas of Natural Language Processing. Also, it's a great metric to be used for imbalanced dataset\n","\n","Link: https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a\n"],"metadata":{"id":"-5jscIM8R4Gv"}},{"cell_type":"code","source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"],"metadata":{"id":"cRaZQ4XC7kLs","outputId":"4b3683d6-0f98-4b33-f11f-f4f2a1442b41","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201391716,"user_tz":-120,"elapsed":230,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating Matthews Corr. Coef. for each batch...\n"]}]},{"cell_type":"code","source":["# Combine the predictions for each batch into a single list of 0s and 1s.\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = [item for sublist in true_labels for item in sublist]\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('MCC: %.3f' % mcc)"],"metadata":{"id":"oCYZa1lQ8Jn8","outputId":"0024cae5-6233-44ad-bfc2-5288c617dee4","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201394266,"user_tz":-120,"elapsed":330,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["MCC: 0.828\n"]}]},{"cell_type":"code","source":["#for i in range(len(flat_true_labels)):\n","#  print(flat_true_labels[i], flat_predictions[i])\n","print(len(flat_predictions))\n","unique, counts = np.unique(flat_predictions, return_counts=True)\n","#unique, counts = np.unique(flat_true_labels, return_counts=True)\n","dict(zip(unique, counts))\n","\n","# 0 = NonArg\n","# 1 = UnVerif\n","# 2 = Verif"],"metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"aNo9EERdbNgM","executionInfo":{"status":"ok","timestamp":1657201396024,"user_tz":-120,"elapsed":216,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"2b8c1a2f-441d-4139-8e44-ce853989910e"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["638\n"]},{"output_type":"execute_result","data":{"text/plain":["{0: 524, 1: 114}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["def show_plot(cm, labels):\n","        ax= plt.subplot()\n","        sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n","\n","        # labels, title and ticks\n","        ax.set_xlabel('Predicted Labels');ax.set_ylabel('True Labels'); \n","        ax.set_title('Confusion Matrix'); \n","        ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n","        plt.show()"],"metadata":{"id":"cqkYrvmxdzQK","executionInfo":{"status":"ok","timestamp":1657201398982,"user_tz":-120,"elapsed":2,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","accuracy = accuracy_score(flat_true_labels, flat_predictions)\n","print(\"Accuracy: {}\".format(accuracy))\n","\n","macro = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='macro')\n","micro = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","weighted = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='weighted')\n","\n","print(\"micro: precision = {} \\t recall = {} \\t f1 = {}\".format(micro[0], micro[1], micro[2]))\n","print(\"macro: precision = {} \\t recall = {} \\t f1 = {}\".format(macro[0], macro[1], macro[2]))\n","print(\"weighted: precision = {} \\t recall = {} \\t f1 = {}\".format(weighted[0], weighted[1], weighted[2]))\n","\n","per_class = precision_recall_fscore_support(flat_true_labels, flat_predictions, average=None, labels=[0, 1, 2])\n","print(\"Per Class:\")\n","print(\"NonArg: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][0], per_class[1][0], per_class[2][0]))\n","print(\"UnVerif: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][1], per_class[1][1], per_class[2][1]))\n","print(\"Verif: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][2], per_class[1][2], per_class[2][2]))\n","\n","cm = confusion_matrix(flat_true_labels, flat_predictions)\n","show_plot(cm, ['NonArg', 'UnVerif', 'Verif'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"id":"4oDkwP-Md16R","executionInfo":{"status":"ok","timestamp":1657201406072,"user_tz":-120,"elapsed":686,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"1b0b5e91-f8b8-4a59-d59c-423bf9fa7e38"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9482758620689655\n","micro: precision = 0.9482758620689655 \t recall = 0.9482758620689655 \t f1 = 0.9482758620689655\n","macro: precision = 0.923898486674702 \t recall = 0.9047828380516969 \t f1 = 0.9139370899503342\n","weighted: precision = 0.9474434638847713 \t recall = 0.9482758620689655 \t f1 = 0.9476794069701306\n","Per Class:\n","NonArg: precision = 0.9618320610687023 \t recall = 0.9748549323017408 \t f1 = 0.968299711815562\n","UnVerif: precision = 0.8859649122807017 \t recall = 0.8347107438016529 \t f1 = 0.8595744680851064\n","Verif: precision = 0.0 \t recall = 0.0 \t f1 = 0.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}