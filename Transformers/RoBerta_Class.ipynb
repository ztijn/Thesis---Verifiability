{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"RoBerta_Class.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"1c57229da36945cd815f5b45f62e1644":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_290eb1a1b9b147eea55e3f9760374b28","IPY_MODEL_58ee324ad633481ea2da7a9652cc8555","IPY_MODEL_75b2d53927fb407c9148daab655e3f24"],"layout":"IPY_MODEL_53af51e57c824a3eaf18a73067f8a48b"}},"290eb1a1b9b147eea55e3f9760374b28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abb7203195a44f05ac91c54357367efe","placeholder":"​","style":"IPY_MODEL_b42c70618b304d20b635bb39e8724f4b","value":"Downloading: 100%"}},"58ee324ad633481ea2da7a9652cc8555":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fedba2e75d64035a69c5c490fc588e7","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8c95b1ecd574ee39f32352d7c161b11","value":898823}},"75b2d53927fb407c9148daab655e3f24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f5c069f3ccc4a39a21448be1b9df890","placeholder":"​","style":"IPY_MODEL_6e8550210b644c23a313b7528405c443","value":" 878k/878k [00:00&lt;00:00, 12.5MB/s]"}},"53af51e57c824a3eaf18a73067f8a48b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abb7203195a44f05ac91c54357367efe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b42c70618b304d20b635bb39e8724f4b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fedba2e75d64035a69c5c490fc588e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8c95b1ecd574ee39f32352d7c161b11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f5c069f3ccc4a39a21448be1b9df890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e8550210b644c23a313b7528405c443":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff45a52029f44e1597113239f0c72e37":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c731e7292bba4e369ad1756fac853997","IPY_MODEL_52aed22731e545b89a91da95bf9eb0f8","IPY_MODEL_795dc453ed3c4383b35a40811c65ac8d"],"layout":"IPY_MODEL_01f8952d0b074b118b9d89cdef5600f2"}},"c731e7292bba4e369ad1756fac853997":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d79060901e2e4a4ca25c1f993bfd7f3d","placeholder":"​","style":"IPY_MODEL_33cd97a6c2fb46528654226dd6746e51","value":"Downloading: 100%"}},"52aed22731e545b89a91da95bf9eb0f8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc5fc1a7f44246309efd42d77d18d860","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_82cdf65e8e3d4574aea3066415fa54ad","value":456318}},"795dc453ed3c4383b35a40811c65ac8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e5fb07e3b9c4323953e6c493d618dc6","placeholder":"​","style":"IPY_MODEL_4c6925f8dc074cc5b33374f1280dce0d","value":" 446k/446k [00:00&lt;00:00, 7.59MB/s]"}},"01f8952d0b074b118b9d89cdef5600f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d79060901e2e4a4ca25c1f993bfd7f3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33cd97a6c2fb46528654226dd6746e51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc5fc1a7f44246309efd42d77d18d860":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82cdf65e8e3d4574aea3066415fa54ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4e5fb07e3b9c4323953e6c493d618dc6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c6925f8dc074cc5b33374f1280dce0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0459ee5e7a244d179abd12cec4844cc6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e2f3aec238ac408e860151c726438a60","IPY_MODEL_c4e8b33e34984c2c981a6cf3ff4c3cf5","IPY_MODEL_318247d3455845038d0a44ad0bf85799"],"layout":"IPY_MODEL_3c07d29c764c40bd99cc27faa3d798ab"}},"e2f3aec238ac408e860151c726438a60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed4e2e4ce14d403f8508bdc6d45ee887","placeholder":"​","style":"IPY_MODEL_4178a3fbf44245dab418d2cf2a79be3b","value":"Downloading: 100%"}},"c4e8b33e34984c2c981a6cf3ff4c3cf5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ae8580c0a0749aaa0cfdfcdff2d1500","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_468d6a13282a4328b5a3aded1129b826","value":481}},"318247d3455845038d0a44ad0bf85799":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3508a7f83286455c824e102bbac2171c","placeholder":"​","style":"IPY_MODEL_1722b63015c44af499ccbfaf326367da","value":" 481/481 [00:00&lt;00:00, 14.1kB/s]"}},"3c07d29c764c40bd99cc27faa3d798ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed4e2e4ce14d403f8508bdc6d45ee887":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4178a3fbf44245dab418d2cf2a79be3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ae8580c0a0749aaa0cfdfcdff2d1500":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"468d6a13282a4328b5a3aded1129b826":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3508a7f83286455c824e102bbac2171c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1722b63015c44af499ccbfaf326367da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3fed907f1604685b8e6ada973f40bb8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ad69c7d5d1043089c2014a38bb6ac48","IPY_MODEL_af55e071b2474cc38ec8d7cc131965b2","IPY_MODEL_136bd3ade74a4fb3b152f9cafa9b74fb"],"layout":"IPY_MODEL_bac7ef00e63e411fa4a5cab42374ce02"}},"0ad69c7d5d1043089c2014a38bb6ac48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ecc6043a6fe42c2a3ea5ef819672f14","placeholder":"​","style":"IPY_MODEL_b0d9d7d604dd4f9d8df98393cde670c7","value":"Downloading: 100%"}},"af55e071b2474cc38ec8d7cc131965b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_091b34fbb0b9453eb30161ecec043e4b","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e26e810bcd84027a3b91903ec1cfdca","value":501200538}},"136bd3ade74a4fb3b152f9cafa9b74fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c005e564d75461a889fc252541aeead","placeholder":"​","style":"IPY_MODEL_d3a57456fcd14057ba589b8a094737d5","value":" 478M/478M [00:09&lt;00:00, 37.7MB/s]"}},"bac7ef00e63e411fa4a5cab42374ce02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ecc6043a6fe42c2a3ea5ef819672f14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0d9d7d604dd4f9d8df98393cde670c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"091b34fbb0b9453eb30161ecec043e4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e26e810bcd84027a3b91903ec1cfdca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c005e564d75461a889fc252541aeead":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3a57456fcd14057ba589b8a094737d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["#Sentence Classification using BERT"],"metadata":{"id":"EKOTlwcmxmej"}},{"cell_type":"code","source":["import tensorflow as tf\n","# Verifying GPU availability (you have to turn it on in google colab)\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"metadata":{"id":"DEfSbAA4QHas","outputId":"5284d369-01ea-4274-938b-0a73f3076516","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201459029,"user_tz":-120,"elapsed":7753,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\")"],"metadata":{"id":"oYsV4H8fCpZ-","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201463780,"user_tz":-120,"elapsed":3380,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"0NmMdkZO8R6q","outputId":"debd40c5-9251-4ec9-a727-ae3cf5fc1f79","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201475891,"user_tz":-120,"elapsed":11062,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 12.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 54.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 55.3 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 13.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_UkeC7SG2krJ","outputId":"3db4afc1-b58b-4336-b589-3973b95ac833","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201493964,"user_tz":-120,"elapsed":16272,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Loading csv file as df and removing irrelevant / empty (= multiple frames) frames\n","train_df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/train_set.csv')\n","validation_df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/val_set.csv')"],"metadata":{"id":"wj2xpqX7exO1","executionInfo":{"status":"ok","timestamp":1657201495955,"user_tz":-120,"elapsed":928,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(train_df.shape)\n","print(validation_df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hgs15asBe_Fr","executionInfo":{"status":"ok","timestamp":1657201497031,"user_tz":-120,"elapsed":215,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"1c7b5b3a-20fb-4005-9fa2-ebc81597634d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(7041, 13)\n","(1123, 13)\n"]}]},{"cell_type":"code","source":["from sklearn import preprocessing\n","\n","# Creating list of comments and corresponding labels\n","sentences_train = train_df.sentence.values\n","sentences_val = validation_df.sentence.values\n","\n","# Getting labels           \n","labels_train = train_df.verif.values\n","labels_val = validation_df.verif.values\n","\n","# Getting unique labels \n","un_labels = set()\n","for label in labels_train:\n","  un_labels.add(label)\n","print(len(un_labels), un_labels)\n","# Rewriting labels from str to int so it can be 'understood' by the classifier \n","le = preprocessing.LabelEncoder()\n","le.fit(sorted(un_labels))\n","\n","train_labels = le.transform(labels_train)\n","validation_labels = le.transform(labels_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9Bzi-owfIcJ","executionInfo":{"status":"ok","timestamp":1657201500407,"user_tz":-120,"elapsed":211,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"311151a0-49b0-4b10-a92b-bda0abf048f7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["3 {'Verif', 'UnVerif', 'NonArg'}\n"]}]},{"cell_type":"code","source":["from transformers import RobertaTokenizer\n","\n","# Load the Roberta tokenizer.\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)"],"metadata":{"id":"Z474sSC6oe7A","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201629841,"user_tz":-120,"elapsed":837,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["1c57229da36945cd815f5b45f62e1644","290eb1a1b9b147eea55e3f9760374b28","58ee324ad633481ea2da7a9652cc8555","75b2d53927fb407c9148daab655e3f24","53af51e57c824a3eaf18a73067f8a48b","abb7203195a44f05ac91c54357367efe","b42c70618b304d20b635bb39e8724f4b","0fedba2e75d64035a69c5c490fc588e7","f8c95b1ecd574ee39f32352d7c161b11","8f5c069f3ccc4a39a21448be1b9df890","6e8550210b644c23a313b7528405c443","ff45a52029f44e1597113239f0c72e37","c731e7292bba4e369ad1756fac853997","52aed22731e545b89a91da95bf9eb0f8","795dc453ed3c4383b35a40811c65ac8d","01f8952d0b074b118b9d89cdef5600f2","d79060901e2e4a4ca25c1f993bfd7f3d","33cd97a6c2fb46528654226dd6746e51","dc5fc1a7f44246309efd42d77d18d860","82cdf65e8e3d4574aea3066415fa54ad","4e5fb07e3b9c4323953e6c493d618dc6","4c6925f8dc074cc5b33374f1280dce0d","0459ee5e7a244d179abd12cec4844cc6","e2f3aec238ac408e860151c726438a60","c4e8b33e34984c2c981a6cf3ff4c3cf5","318247d3455845038d0a44ad0bf85799","3c07d29c764c40bd99cc27faa3d798ab","ed4e2e4ce14d403f8508bdc6d45ee887","4178a3fbf44245dab418d2cf2a79be3b","7ae8580c0a0749aaa0cfdfcdff2d1500","468d6a13282a4328b5a3aded1129b826","3508a7f83286455c824e102bbac2171c","1722b63015c44af499ccbfaf326367da"]},"outputId":"4bd4b99c-3641-451c-b902-4857dab671f9"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c57229da36945cd815f5b45f62e1644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff45a52029f44e1597113239f0c72e37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0459ee5e7a244d179abd12cec4844cc6"}},"metadata":{}}]},{"cell_type":"code","source":["def sent_to_id(sentences):\n","  # Tokenize all of the sentences and map the tokens to thier word IDs.\n","  input_ids = []\n","\n","  # For every sentence...\n","  for sent in sentences:\n","      # `encode` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      encoded_sent = tokenizer.encode(\n","                          sent\n","                    )\n","      \n","      # Add the encoded sentence to the list.\n","      input_ids.append(encoded_sent)\n","\n","  # Print sentence 0, now as a list of IDs.\n","  print('Original: ', sentences[0])\n","  print('Token IDs:', input_ids[0])\n","  return(input_ids)"],"metadata":{"id":"2bBdb3pt8LuQ","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201717170,"user_tz":-120,"elapsed":289,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["train_inputs = sent_to_id(sentences_train)\n","validation_inputs = sent_to_id(sentences_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Yj-NSQ8gGEX","executionInfo":{"status":"ok","timestamp":1657201721927,"user_tz":-120,"elapsed":2536,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"b95ebf68-3a40-40dc-bda7-71e298d6b20f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  I have so many responses to this, but most of them stem from emotions, so I'm going to put them aside, and just go with two.\n","Token IDs: [0, 38, 33, 98, 171, 8823, 7, 42, 6, 53, 144, 9, 106, 10114, 31, 8597, 6, 98, 38, 437, 164, 7, 342, 106, 4364, 6, 8, 95, 213, 19, 80, 4, 2]\n","Original:  You don't make any attempt to cite your claims.\n","Token IDs: [0, 370, 218, 75, 146, 143, 2120, 7, 22884, 110, 1449, 4, 2]\n"]}]},{"cell_type":"code","source":["print('Max sentence length(train): ', max([len(sen) for sen in train_inputs]))\n","print('Max sentence length(validation): ', max([len(sen) for sen in validation_inputs]))"],"metadata":{"id":"JhUZO9vc_l6T","outputId":"53586a20-8593-46ef-a237-9485c5af5342","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201729180,"user_tz":-120,"elapsed":205,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sentence length(train):  223\n","Max sentence length(validation):  128\n"]}]},{"cell_type":"code","source":["# We will use some utility function from tensorflow(Tensorflow was my first crush)\n","from keras.preprocessing.sequence import pad_sequences\n","\n","MAX_LEN = 305\n","\n","#Padding the input to the max length that is 64\n","train_inputs = pad_sequences(train_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","validation_inputs = pad_sequences(validation_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")"],"metadata":{"id":"Cp9BPRd1tMIo","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201735348,"user_tz":-120,"elapsed":214,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def attentionmasks(input_ids):\n","  # Creating the attention masks\n","  attention_masks = []\n","\n","  # For each sentence...\n","  for sent in input_ids:\n","      \n","      # Create the attention mask.\n","      #   - If a token ID is 0, then it's padding, set the mask to 0.\n","      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","      att_mask = [int(token_id > 0) for token_id in sent]\n","      \n","      # Store the attention mask for this sentence.\n","      attention_masks.append(att_mask)\n","  return(attention_masks)"],"metadata":{"id":"cDoC24LeEv3N","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201745687,"user_tz":-120,"elapsed":215,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["train_masks = attentionmasks(train_inputs)\n","validation_masks = attentionmasks(validation_inputs)"],"metadata":{"id":"-l_lHhwGiSiX","executionInfo":{"status":"ok","timestamp":1657201748538,"user_tz":-120,"elapsed":1206,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#Converting the input data to the tensor , which can be feeded to the model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"metadata":{"id":"jw5K2A5Ko1RF","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201750402,"user_tz":-120,"elapsed":235,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#Creating the DataLoader which will help us to load data into the GPU/CPU\n","batch_size = 32\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"],"metadata":{"id":"GEgLpFVlo1Z-","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201753744,"user_tz":-120,"elapsed":226,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["#Loading the pre-trained BERT model from huggingface library\n","\n","from transformers import RobertaForSequenceClassification, AdamW, RobertaConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = RobertaForSequenceClassification.from_pretrained(\n","    \"roberta-base\", \n","    num_labels = 3,   \n","    output_attentions = False, \n","    output_hidden_states = False, )\n","\n","# Teeling the model to run on GPU\n","model.cuda()"],"metadata":{"id":"gFsCTp_mporB","outputId":"bd57ce59-af15-4db4-986f-364012e7f60a","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a3fed907f1604685b8e6ada973f40bb8","0ad69c7d5d1043089c2014a38bb6ac48","af55e071b2474cc38ec8d7cc131965b2","136bd3ade74a4fb3b152f9cafa9b74fb","bac7ef00e63e411fa4a5cab42374ce02","7ecc6043a6fe42c2a3ea5ef819672f14","b0d9d7d604dd4f9d8df98393cde670c7","091b34fbb0b9453eb30161ecec043e4b","9e26e810bcd84027a3b91903ec1cfdca","5c005e564d75461a889fc252541aeead","d3a57456fcd14057ba589b8a094737d5"]},"executionInfo":{"status":"ok","timestamp":1657201863064,"user_tz":-120,"elapsed":22388,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3fed907f1604685b8e6ada973f40bb8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# AdamW is an optimizer which is a Adam Optimzier with weight-decay-fix\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, \n","                  eps = 1e-8 \n","                )\n"],"metadata":{"id":"GLs72DuMODJO","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201866865,"user_tz":-120,"elapsed":301,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"a168083d-ccdf-47b7-9153-37b9465c9104"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","scheduler"],"metadata":{"id":"-p0upAhhRiIx","outputId":"41024644-e9d3-4aa8-d567-945c9cc6ebfd","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657201871031,"user_tz":-120,"elapsed":211,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.optim.lr_scheduler.LambdaLR at 0x7f90a5f6a6d0>"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["Define a helper function for calculating accuracy."],"metadata":{"id":"pE5B99H5H2-W"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"9cQNvaZ9bnyy","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201880083,"user_tz":-120,"elapsed":212,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["#Creating the helper function to have a watch on elapsed time\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"metadata":{"id":"gpt6tR83keZD","trusted":true,"executionInfo":{"status":"ok","timestamp":1657201883170,"user_tz":-120,"elapsed":193,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#Let's start the training process\n","\n","import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"metadata":{"id":"6J-FYdx6nFE_","outputId":"68a47479-d226-453e-8642-1573c6de3422","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203416885,"user_tz":-120,"elapsed":1529877,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    221.    Elapsed: 0:01:00.\n","  Batch    80  of    221.    Elapsed: 0:02:02.\n","  Batch   120  of    221.    Elapsed: 0:03:07.\n","  Batch   160  of    221.    Elapsed: 0:04:11.\n","  Batch   200  of    221.    Elapsed: 0:05:17.\n","\n","  Average training loss: 0.71\n","  Training epoch took: 0:05:49\n","\n","Running Validation...\n","  Accuracy: 0.79\n","  Validation took: 0:00:23\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    221.    Elapsed: 0:01:06.\n","  Batch    80  of    221.    Elapsed: 0:02:12.\n","  Batch   120  of    221.    Elapsed: 0:03:18.\n","  Batch   160  of    221.    Elapsed: 0:04:24.\n","  Batch   200  of    221.    Elapsed: 0:05:30.\n","\n","  Average training loss: 0.55\n","  Training epoch took: 0:06:03\n","\n","Running Validation...\n","  Accuracy: 0.78\n","  Validation took: 0:00:23\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    221.    Elapsed: 0:01:06.\n","  Batch    80  of    221.    Elapsed: 0:02:12.\n","  Batch   120  of    221.    Elapsed: 0:03:18.\n","  Batch   160  of    221.    Elapsed: 0:04:24.\n","  Batch   200  of    221.    Elapsed: 0:05:29.\n","\n","  Average training loss: 0.47\n","  Training epoch took: 0:06:02\n","\n","Running Validation...\n","  Accuracy: 0.77\n","  Validation took: 0:00:23\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    221.    Elapsed: 0:01:06.\n","  Batch    80  of    221.    Elapsed: 0:02:12.\n","  Batch   120  of    221.    Elapsed: 0:03:18.\n","  Batch   160  of    221.    Elapsed: 0:04:23.\n","  Batch   200  of    221.    Elapsed: 0:05:29.\n","\n","  Average training loss: 0.40\n","  Training epoch took: 0:06:02\n","\n","Running Validation...\n","  Accuracy: 0.77\n","  Validation took: 0:00:23\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","source":["25 min training time"],"metadata":{"id":"4d2MZ8YtqmvO"}},{"cell_type":"code","source":["print(loss_values) #Having a view of stored loss values in the list"],"metadata":{"id":"btUsZ5vMyjwt","outputId":"2c2fe851-83b9-402d-aff2-bfb64255ffc7","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203506635,"user_tz":-120,"elapsed":194,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.707820167166615, 0.5500806429806877, 0.4673002102660917, 0.40309170761663987]\n"]}]},{"cell_type":"code","source":["#Loading the test data and applying the same preprocessing techniques which we performed on the train data\n","import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/test_set.csv')\n","\n","# Report the number of sentences.\n","print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Create sentence and label lists\n","sentences = df.sentence.values\n","labels = df.verif.values\n","\n","labels = le.transform(labels)\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                   )\n","    \n","    input_ids.append(encoded_sent)\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","# Convert to tensors.\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"mAN0LZBOOPVh","outputId":"e12c40b3-8a94-44f4-fe9d-02c3f5208133","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203514178,"user_tz":-120,"elapsed":1463,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of test sentences: 1,962\n","\n"]}]},{"cell_type":"code","source":["#Evaluating our model on the test set\n","\n","# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n"],"metadata":{"id":"Hba10sXR7Xi6","outputId":"113efd58-0e6f-4cff-e5ff-2b890b95ea47","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203553701,"user_tz":-120,"elapsed":36936,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 1,962 test sentences...\n"]}]},{"cell_type":"markdown","source":["We will use Matthews Correlation Coefficient(MCC) to evaluate our model. \n","MCC is used in many areas of Natural Language Processing. Also, it's a great metric to be used for imbalanced dataset\n","\n","Link: https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a\n"],"metadata":{"id":"-5jscIM8R4Gv"}},{"cell_type":"code","source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"],"metadata":{"id":"cRaZQ4XC7kLs","outputId":"892ca8ca-d9c8-4a18-ea73-ec267d5d7792","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203557332,"user_tz":-120,"elapsed":2,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating Matthews Corr. Coef. for each batch...\n"]}]},{"cell_type":"code","source":["# Combine the predictions for each batch into a single list of 0s and 1s.\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = [item for sublist in true_labels for item in sublist]\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('MCC: %.3f' % mcc)"],"metadata":{"id":"oCYZa1lQ8Jn8","outputId":"ac03ec98-f43c-48a7-c930-5ac63bad1b55","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203559001,"user_tz":-120,"elapsed":191,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["MCC: 0.563\n"]}]},{"cell_type":"code","source":["#for i in range(len(flat_true_labels)):\n","#  print(flat_true_labels[i], flat_predictions[i])\n","print(len(flat_predictions))\n","unique, counts = np.unique(flat_predictions, return_counts=True)\n","#unique, counts = np.unique(flat_true_labels, return_counts=True)\n","dict(zip(unique, counts))\n","\n","# 0 = NonArg\n","# 1 = UnVerif\n","# 2 = Verif"],"metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"aNo9EERdbNgM","executionInfo":{"status":"ok","timestamp":1657203563921,"user_tz":-120,"elapsed":212,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"2969b535-6f71-4aa3-8b12-f04a258434d6"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["1962\n"]},{"output_type":"execute_result","data":{"text/plain":["{0: 230, 1: 1215, 2: 517}"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["def show_plot(cm, labels):\n","        ax= plt.subplot()\n","        sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n","\n","        # labels, title and ticks\n","        ax.set_xlabel('Predicted Labels');ax.set_ylabel('True Labels'); \n","        ax.set_title('Confusion Matrix'); \n","        ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n","        plt.show()"],"metadata":{"id":"cqkYrvmxdzQK","executionInfo":{"status":"ok","timestamp":1657203565944,"user_tz":-120,"elapsed":2,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","accuracy = accuracy_score(flat_true_labels, flat_predictions)\n","print(\"Accuracy: {}\".format(accuracy))\n","\n","macro = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='macro')\n","micro = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","weighted = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='weighted')\n","\n","print(\"micro: precision = {} \\t recall = {} \\t f1 = {}\".format(micro[0], micro[1], micro[2]))\n","print(\"macro: precision = {} \\t recall = {} \\t f1 = {}\".format(macro[0], macro[1], macro[2]))\n","print(\"weighted: precision = {} \\t recall = {} \\t f1 = {}\".format(weighted[0], weighted[1], weighted[2]))\n","\n","per_class = precision_recall_fscore_support(flat_true_labels, flat_predictions, average=None, labels=[0, 1, 2])\n","print(\"Per Class:\")\n","print(\"NonArg: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][0], per_class[1][0], per_class[2][0]))\n","print(\"UnVerif: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][1], per_class[1][1], per_class[2][1]))\n","print(\"Verif: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][2], per_class[1][2], per_class[2][2]))\n","\n","cm = confusion_matrix(flat_true_labels, flat_predictions)\n","show_plot(cm, ['NonArg', 'UnVerif', 'Verif'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"id":"4oDkwP-Md16R","executionInfo":{"status":"ok","timestamp":1657203571181,"user_tz":-120,"elapsed":711,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"599ab659-f0d0-4fd0-9d77-681d97d0a795"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7477064220183486\n","micro: precision = 0.7477064220183486 \t recall = 0.7477064220183486 \t f1 = 0.7477064220183486\n","macro: precision = 0.7859439866395018 \t recall = 0.7247326388847878 \t f1 = 0.7484618039115986\n","weighted: precision = 0.7511847724133858 \t recall = 0.7477064220183486 \t f1 = 0.7433733233407162\n","Per Class:\n","NonArg: precision = 0.9173913043478261 \t recall = 0.7455830388692579 \t f1 = 0.8226120857699805\n","UnVerif: precision = 0.7325102880658436 \t recall = 0.8549471661863592 \t f1 = 0.7890070921985816\n","Verif: precision = 0.7079303675048356 \t recall = 0.5736677115987461 \t f1 = 0.6337662337662338\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}