{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"RoBerta_Class_Pers.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"6150a47a201240d9aac8ade5a74b570a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_74dee197c41944049d51db0c55619777","IPY_MODEL_67e0aab0730542e1bf9feabe7ac8c612","IPY_MODEL_a7cbcc30dee84fb5ab5d3fdbf8f62bd9"],"layout":"IPY_MODEL_4025d4b5981142cf80439ddf62fd88b1"}},"74dee197c41944049d51db0c55619777":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3a3b00942f84c77a4da4538320f176a","placeholder":"​","style":"IPY_MODEL_844828aba9cc4c0a8979914056f2153d","value":"Downloading: 100%"}},"67e0aab0730542e1bf9feabe7ac8c612":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_13a6e8de222148aab459a403454c9ea6","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7b02eec689e4897aaaf25d609ae42c2","value":898823}},"a7cbcc30dee84fb5ab5d3fdbf8f62bd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58e8426a383641e18b48c081764faf48","placeholder":"​","style":"IPY_MODEL_791c4d366ef54d1cbc29630809664d4f","value":" 878k/878k [00:00&lt;00:00, 1.80MB/s]"}},"4025d4b5981142cf80439ddf62fd88b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3a3b00942f84c77a4da4538320f176a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"844828aba9cc4c0a8979914056f2153d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13a6e8de222148aab459a403454c9ea6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7b02eec689e4897aaaf25d609ae42c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58e8426a383641e18b48c081764faf48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"791c4d366ef54d1cbc29630809664d4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5dfd9df950c4944ae97ad825e2d3426":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4b90759f53c4d8eb8b9870cb47590cb","IPY_MODEL_f1bc1205952f439d82f15b8a10436612","IPY_MODEL_b49a654e790643608517e991d7b38705"],"layout":"IPY_MODEL_c4c7ef1b60f1426b92312cea3f7a6034"}},"f4b90759f53c4d8eb8b9870cb47590cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef6833cc89804d6dad3435840cf1e9d5","placeholder":"​","style":"IPY_MODEL_f31c5b08f3fe4ce6a655383bd56eb6ec","value":"Downloading: 100%"}},"f1bc1205952f439d82f15b8a10436612":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b920b3c24da4ba086fb8cd16fe6b72a","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ebaf9640d7ee4b169a10a6b0ea5f160f","value":456318}},"b49a654e790643608517e991d7b38705":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0284496cab1c4a7bae3f1bc5f8567b0a","placeholder":"​","style":"IPY_MODEL_5c5d7ea961cb400ca6f932a6d1aeffb8","value":" 446k/446k [00:00&lt;00:00, 1.36MB/s]"}},"c4c7ef1b60f1426b92312cea3f7a6034":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef6833cc89804d6dad3435840cf1e9d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f31c5b08f3fe4ce6a655383bd56eb6ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b920b3c24da4ba086fb8cd16fe6b72a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebaf9640d7ee4b169a10a6b0ea5f160f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0284496cab1c4a7bae3f1bc5f8567b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c5d7ea961cb400ca6f932a6d1aeffb8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfaf589f269b47dfa338e0a8441e35b8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d39d9a5d94e403cba4a46655d22b6ef","IPY_MODEL_f0518bee947348cd99ae999a55bb8bd6","IPY_MODEL_2a3cd84154ca4ebf9a72146993afb9ec"],"layout":"IPY_MODEL_707b46b6a19640f3a087a561d2049dce"}},"9d39d9a5d94e403cba4a46655d22b6ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dea086d4fd4342429e941092d4e27122","placeholder":"​","style":"IPY_MODEL_8f1e6944ee4a49c690eac54b5e01b5cc","value":"Downloading: 100%"}},"f0518bee947348cd99ae999a55bb8bd6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ead39a3ff6543fc98170dda90b737ad","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f2fd8bfa08b140e49a11436f85f95ab5","value":481}},"2a3cd84154ca4ebf9a72146993afb9ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b79675b7f76f4b54bc01ce0b0096b888","placeholder":"​","style":"IPY_MODEL_0948d3fc6ca34a6bb0a0a10f9b6613a8","value":" 481/481 [00:00&lt;00:00, 14.8kB/s]"}},"707b46b6a19640f3a087a561d2049dce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dea086d4fd4342429e941092d4e27122":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f1e6944ee4a49c690eac54b5e01b5cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ead39a3ff6543fc98170dda90b737ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2fd8bfa08b140e49a11436f85f95ab5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b79675b7f76f4b54bc01ce0b0096b888":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0948d3fc6ca34a6bb0a0a10f9b6613a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b4a4bae43bc49d3a3c27f4896306744":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_305acadfa442471cb3d860138a465347","IPY_MODEL_928982e8cf7e455abfa2ed082e669b1c","IPY_MODEL_a8292362ad9c42ab9e8fd15b0490cbf8"],"layout":"IPY_MODEL_f455ee4d78e44d3c99c5b4c454de8140"}},"305acadfa442471cb3d860138a465347":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c491efd2a460488fbd32f318804b921c","placeholder":"​","style":"IPY_MODEL_8ed7a395362c488983b99e7b3086225a","value":"Downloading: 100%"}},"928982e8cf7e455abfa2ed082e669b1c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf45d978647a460298cd8c4253341d06","max":501200538,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a641af63070415b904e7c86668fde2f","value":501200538}},"a8292362ad9c42ab9e8fd15b0490cbf8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d4d77106f5464eb8b6e459a424f54164","placeholder":"​","style":"IPY_MODEL_c9e7ef50bba84ea2a702b435399c4577","value":" 478M/478M [00:08&lt;00:00, 62.5MB/s]"}},"f455ee4d78e44d3c99c5b4c454de8140":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c491efd2a460488fbd32f318804b921c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ed7a395362c488983b99e7b3086225a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf45d978647a460298cd8c4253341d06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a641af63070415b904e7c86668fde2f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d4d77106f5464eb8b6e459a424f54164":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9e7ef50bba84ea2a702b435399c4577":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["#Sentence Classification using BERT"],"metadata":{"id":"EKOTlwcmxmej"}},{"cell_type":"code","source":["import tensorflow as tf\n","# Verifying GPU availability (you have to turn it on in google colab)\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"metadata":{"id":"DEfSbAA4QHas","outputId":"c5e74f3c-2c28-477b-b433-ae84ea13f19d","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203610239,"user_tz":-120,"elapsed":7917,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\")"],"metadata":{"id":"oYsV4H8fCpZ-","trusted":true,"executionInfo":{"status":"ok","timestamp":1657203634725,"user_tz":-120,"elapsed":3549,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"0NmMdkZO8R6q","outputId":"fef0f3c1-c88b-4194-ea0c-defd7010ff1a","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203645536,"user_tz":-120,"elapsed":10046,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 47.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 13.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 66.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_UkeC7SG2krJ","outputId":"c8b9223d-6742-4539-abca-105b1c7a1462","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203663585,"user_tz":-120,"elapsed":15920,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Loading csv file as df and removing irrelevant / empty (= multiple frames) frames\n","train_df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/train_set_verif.csv')\n","validation_df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/val_set_verif.csv')"],"metadata":{"id":"wj2xpqX7exO1","executionInfo":{"status":"ok","timestamp":1657203666381,"user_tz":-120,"elapsed":982,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(train_df.shape)\n","print(validation_df.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hgs15asBe_Fr","executionInfo":{"status":"ok","timestamp":1657203667953,"user_tz":-120,"elapsed":2,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"8d8b2d9a-0f7e-4897-9b9f-770edd5233a9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(1870, 13)\n","(330, 13)\n"]}]},{"cell_type":"code","source":["from sklearn import preprocessing\n","\n","# Creating list of comments and corresponding labels\n","sentences_train = train_df.sentence.values\n","sentences_val = validation_df.sentence.values\n","\n","# Getting labels           \n","labels_train = train_df.personal.values\n","labels_val = validation_df.personal.values\n","\n","# Getting unique labels \n","un_labels = set()\n","for label in labels_train:\n","  un_labels.add(label)\n","print(len(un_labels), un_labels)\n","# Rewriting labels from str to int so it can be 'understood' by the classifier \n","le = preprocessing.LabelEncoder()\n","le.fit(sorted(un_labels))\n","\n","train_labels = le.transform(labels_train)\n","validation_labels = le.transform(labels_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h9Bzi-owfIcJ","executionInfo":{"status":"ok","timestamp":1657203670349,"user_tz":-120,"elapsed":181,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"c97773c5-bc18-40b4-f309-8450fad3789e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2 {'Pers', 'NonPers'}\n"]}]},{"cell_type":"code","source":["from transformers import RobertaTokenizer\n","\n","# Load the Roberta tokenizer.\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", add_prefix_space=True)"],"metadata":{"id":"Z474sSC6oe7A","trusted":true,"executionInfo":{"status":"ok","timestamp":1657203675332,"user_tz":-120,"elapsed":1960,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["6150a47a201240d9aac8ade5a74b570a","74dee197c41944049d51db0c55619777","67e0aab0730542e1bf9feabe7ac8c612","a7cbcc30dee84fb5ab5d3fdbf8f62bd9","4025d4b5981142cf80439ddf62fd88b1","d3a3b00942f84c77a4da4538320f176a","844828aba9cc4c0a8979914056f2153d","13a6e8de222148aab459a403454c9ea6","a7b02eec689e4897aaaf25d609ae42c2","58e8426a383641e18b48c081764faf48","791c4d366ef54d1cbc29630809664d4f","a5dfd9df950c4944ae97ad825e2d3426","f4b90759f53c4d8eb8b9870cb47590cb","f1bc1205952f439d82f15b8a10436612","b49a654e790643608517e991d7b38705","c4c7ef1b60f1426b92312cea3f7a6034","ef6833cc89804d6dad3435840cf1e9d5","f31c5b08f3fe4ce6a655383bd56eb6ec","7b920b3c24da4ba086fb8cd16fe6b72a","ebaf9640d7ee4b169a10a6b0ea5f160f","0284496cab1c4a7bae3f1bc5f8567b0a","5c5d7ea961cb400ca6f932a6d1aeffb8","dfaf589f269b47dfa338e0a8441e35b8","9d39d9a5d94e403cba4a46655d22b6ef","f0518bee947348cd99ae999a55bb8bd6","2a3cd84154ca4ebf9a72146993afb9ec","707b46b6a19640f3a087a561d2049dce","dea086d4fd4342429e941092d4e27122","8f1e6944ee4a49c690eac54b5e01b5cc","8ead39a3ff6543fc98170dda90b737ad","f2fd8bfa08b140e49a11436f85f95ab5","b79675b7f76f4b54bc01ce0b0096b888","0948d3fc6ca34a6bb0a0a10f9b6613a8"]},"outputId":"a5bcd966-3175-4b45-944e-c0802781b60a"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6150a47a201240d9aac8ade5a74b570a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5dfd9df950c4944ae97ad825e2d3426"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfaf589f269b47dfa338e0a8441e35b8"}},"metadata":{}}]},{"cell_type":"code","source":["def sent_to_id(sentences):\n","  # Tokenize all of the sentences and map the tokens to thier word IDs.\n","  input_ids = []\n","\n","  # For every sentence...\n","  for sent in sentences:\n","      # `encode` will:\n","      #   (1) Tokenize the sentence.\n","      #   (2) Prepend the `[CLS]` token to the start.\n","      #   (3) Append the `[SEP]` token to the end.\n","      #   (4) Map tokens to their IDs.\n","      encoded_sent = tokenizer.encode(\n","                          sent\n","                    )\n","      \n","      # Add the encoded sentence to the list.\n","      input_ids.append(encoded_sent)\n","\n","  # Print sentence 0, now as a list of IDs.\n","  print('Original: ', sentences[0])\n","  print('Token IDs:', input_ids[0])\n","  return(input_ids)"],"metadata":{"id":"2bBdb3pt8LuQ","trusted":true,"executionInfo":{"status":"ok","timestamp":1657203678036,"user_tz":-120,"elapsed":164,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["train_inputs = sent_to_id(sentences_train)\n","validation_inputs = sent_to_id(sentences_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Yj-NSQ8gGEX","executionInfo":{"status":"ok","timestamp":1657203680294,"user_tz":-120,"elapsed":1009,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"fcacbf3a-6d70-43d9-f761-6081f94fe392"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  Most of the people in America don't see a dime of social program money until they turn 65, but they start contributing to them at 16 (or whenever they get their first job).\n","Token IDs: [0, 1993, 9, 5, 82, 11, 730, 218, 75, 192, 10, 30979, 9, 592, 586, 418, 454, 51, 1004, 3620, 6, 53, 51, 386, 8216, 7, 106, 23, 545, 36, 368, 8378, 51, 120, 49, 78, 633, 322, 2]\n","Original:  You don't make any attempt to cite your claims.\n","Token IDs: [0, 370, 218, 75, 146, 143, 2120, 7, 22884, 110, 1449, 4, 2]\n"]}]},{"cell_type":"code","source":["print('Max sentence length(train): ', max([len(sen) for sen in train_inputs]))\n","print('Max sentence length(validation): ', max([len(sen) for sen in validation_inputs]))"],"metadata":{"id":"JhUZO9vc_l6T","outputId":"53a91dd7-1ac8-457b-f0fd-c448ec6dff73","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203694959,"user_tz":-120,"elapsed":180,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Max sentence length(train):  130\n","Max sentence length(validation):  128\n"]}]},{"cell_type":"code","source":["# We will use some utility function from tensorflow(Tensorflow was my first crush)\n","from keras.preprocessing.sequence import pad_sequences\n","\n","MAX_LEN = 305\n","\n","#Padding the input to the max length that is 64\n","train_inputs = pad_sequences(train_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n","validation_inputs = pad_sequences(validation_inputs, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")"],"metadata":{"id":"Cp9BPRd1tMIo","trusted":true,"executionInfo":{"status":"ok","timestamp":1657203721447,"user_tz":-120,"elapsed":178,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def attentionmasks(input_ids):\n","  # Creating the attention masks\n","  attention_masks = []\n","\n","  # For each sentence...\n","  for sent in input_ids:\n","      \n","      # Create the attention mask.\n","      #   - If a token ID is 0, then it's padding, set the mask to 0.\n","      #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n","      att_mask = [int(token_id > 0) for token_id in sent]\n","      \n","      # Store the attention mask for this sentence.\n","      attention_masks.append(att_mask)\n","  return(attention_masks)"],"metadata":{"id":"cDoC24LeEv3N","trusted":true,"executionInfo":{"status":"ok","timestamp":1657203723506,"user_tz":-120,"elapsed":177,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["train_masks = attentionmasks(train_inputs)\n","validation_masks = attentionmasks(validation_inputs)"],"metadata":{"id":"-l_lHhwGiSiX","executionInfo":{"status":"ok","timestamp":1657203725842,"user_tz":-120,"elapsed":174,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["#Converting the input data to the tensor , which can be feeded to the model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"],"metadata":{"id":"jw5K2A5Ko1RF","trusted":true,"executionInfo":{"status":"ok","timestamp":1657203726751,"user_tz":-120,"elapsed":1,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","#Creating the DataLoader which will help us to load data into the GPU/CPU\n","batch_size = 32\n","\n","# Create the DataLoader for our training set.\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set.\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"],"metadata":{"id":"GEgLpFVlo1Z-","trusted":true,"executionInfo":{"status":"ok","timestamp":1657203730267,"user_tz":-120,"elapsed":169,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["#Loading the pre-trained BERT model from huggingface library\n","\n","from transformers import RobertaForSequenceClassification, AdamW, RobertaConfig\n","\n","# Load BertForSequenceClassification, the pretrained BERT model with a single \n","# linear classification layer on top. \n","model = RobertaForSequenceClassification.from_pretrained(\n","    \"roberta-base\", \n","    num_labels = 2,   \n","    output_attentions = False, \n","    output_hidden_states = False, )\n","\n","# Teeling the model to run on GPU\n","model.cuda()"],"metadata":{"id":"gFsCTp_mporB","outputId":"1e1d7ffd-d301-4f1a-cdbf-1a4ad1f032ee","trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9b4a4bae43bc49d3a3c27f4896306744","305acadfa442471cb3d860138a465347","928982e8cf7e455abfa2ed082e669b1c","a8292362ad9c42ab9e8fd15b0490cbf8","f455ee4d78e44d3c99c5b4c454de8140","c491efd2a460488fbd32f318804b921c","8ed7a395362c488983b99e7b3086225a","cf45d978647a460298cd8c4253341d06","0a641af63070415b904e7c86668fde2f","d4d77106f5464eb8b6e459a424f54164","c9e7ef50bba84ea2a702b435399c4577"]},"executionInfo":{"status":"ok","timestamp":1657203772765,"user_tz":-120,"elapsed":21312,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b4a4bae43bc49d3a3c27f4896306744"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# AdamW is an optimizer which is a Adam Optimzier with weight-decay-fix\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, \n","                  eps = 1e-8 \n","                )\n"],"metadata":{"id":"GLs72DuMODJO","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203794790,"user_tz":-120,"elapsed":175,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"519759d9-e1c0-4b06-e11c-942b0e2d494a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs (authors recommend between 2 and 4)\n","epochs = 4\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","scheduler"],"metadata":{"id":"-p0upAhhRiIx","outputId":"4882a569-5d26-4603-f2aa-26656d0549e0","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203796076,"user_tz":-120,"elapsed":166,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.optim.lr_scheduler.LambdaLR at 0x7f149bb1a510>"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["Define a helper function for calculating accuracy."],"metadata":{"id":"pE5B99H5H2-W"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"9cQNvaZ9bnyy","trusted":true,"executionInfo":{"status":"ok","timestamp":1657203799082,"user_tz":-120,"elapsed":187,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["#Creating the helper function to have a watch on elapsed time\n","\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"metadata":{"id":"gpt6tR83keZD","trusted":true,"executionInfo":{"status":"ok","timestamp":1657203800788,"user_tz":-120,"elapsed":156,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["#Let's start the training process\n","\n","import random\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Store the average loss after each epoch so we can plot them.\n","loss_values = []\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # This will return the loss (rather than the model output) because we\n","        # have provided the `labels`.\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        outputs = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels)\n","        \n","        # The call to `model` always returns a tuple, so we need to pull the \n","        # loss value out of the tuple.\n","        loss = outputs[0]\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)            \n","    \n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        \n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        # Telling the model not to compute or store gradients, saving memory and\n","        # speeding up validation\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # This will return the logits rather than the loss because we have\n","            # not provided labels.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here: \n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask)\n","        \n","        # Get the \"logits\" output by the model. The \"logits\" are the output\n","        # values prior to applying an activation function like the softmax.\n","        logits = outputs[0]\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        \n","        # Calculate the accuracy for this batch of test sentences.\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        # Accumulate the total accuracy.\n","        eval_accuracy += tmp_eval_accuracy\n","\n","        # Track the number of batches\n","        nb_eval_steps += 1\n","\n","    # Report the final accuracy for this validation run.\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n","\n","print(\"\")\n","print(\"Training complete!\")"],"metadata":{"id":"6J-FYdx6nFE_","outputId":"a0a15b2b-ea43-48c6-f9fd-7a501b953e9e","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657204225825,"user_tz":-120,"elapsed":421024,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:01:02.\n","\n","  Average training loss: 0.45\n","  Training epoch took: 0:01:31\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation took: 0:00:07\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:01:06.\n","\n","  Average training loss: 0.20\n","  Training epoch took: 0:01:37\n","\n","Running Validation...\n","  Accuracy: 0.95\n","  Validation took: 0:00:07\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:01:09.\n","\n","  Average training loss: 0.15\n","  Training epoch took: 0:01:42\n","\n","Running Validation...\n","  Accuracy: 0.96\n","  Validation took: 0:00:07\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of     59.    Elapsed: 0:01:10.\n","\n","  Average training loss: 0.10\n","  Training epoch took: 0:01:42\n","\n","Running Validation...\n","  Accuracy: 0.96\n","  Validation took: 0:00:07\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","source":["7 min training time"],"metadata":{"id":"4d2MZ8YtqmvO"}},{"cell_type":"code","source":["print(loss_values) #Having a view of stored loss values in the list"],"metadata":{"id":"btUsZ5vMyjwt","outputId":"e3623357-94af-4be6-90f1-17d21f0056ed","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657204233543,"user_tz":-120,"elapsed":168,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.44644167261608575, 0.19751434373842963, 0.14843524941953562, 0.10082891261426069]\n"]}]},{"cell_type":"code","source":["#Loading the test data and applying the same preprocessing techniques which we performed on the train data\n","import pandas as pd\n","\n","# Load the dataset into a pandas dataframe.\n","df = pd.read_csv('/content/drive/MyDrive/Studie/Thesis/test_set_verif.csv')\n","\n","# Report the number of sentences.\n","print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Create sentence and label lists\n","sentences = df.sentence.values\n","labels = df.personal.values\n","\n","labels = le.transform(labels)\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    encoded_sent = tokenizer.encode(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                   )\n","    \n","    input_ids.append(encoded_sent)\n","\n","# Pad our input tokens\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n","                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  attention_masks.append(seq_mask) \n","\n","# Convert to tensors.\n","prediction_inputs = torch.tensor(input_ids)\n","prediction_masks = torch.tensor(attention_masks)\n","prediction_labels = torch.tensor(labels)\n","\n","# Set the batch size.  \n","batch_size = 32  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"id":"mAN0LZBOOPVh","outputId":"59851ef8-e52d-481c-8102-cd5f723b9468","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657204236170,"user_tz":-120,"elapsed":705,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of test sentences: 638\n","\n"]}]},{"cell_type":"code","source":["#Evaluating our model on the test set\n","\n","# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n"],"metadata":{"id":"Hba10sXR7Xi6","outputId":"8fe6c6f9-77e7-4853-d51f-45eaa4558eda","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657204255094,"user_tz":-120,"elapsed":14078,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 638 test sentences...\n"]}]},{"cell_type":"markdown","source":["We will use Matthews Correlation Coefficient(MCC) to evaluate our model. \n","MCC is used in many areas of Natural Language Processing. Also, it's a great metric to be used for imbalanced dataset\n","\n","Link: https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a\n"],"metadata":{"id":"-5jscIM8R4Gv"}},{"cell_type":"code","source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"],"metadata":{"id":"cRaZQ4XC7kLs","outputId":"1d37b502-0d79-45f4-f64b-a996dd0d6dcd","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657204257493,"user_tz":-120,"elapsed":167,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating Matthews Corr. Coef. for each batch...\n"]}]},{"cell_type":"code","source":["# Combine the predictions for each batch into a single list of 0s and 1s.\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = [item for sublist in true_labels for item in sublist]\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('MCC: %.3f' % mcc)"],"metadata":{"id":"oCYZa1lQ8Jn8","outputId":"cf0f98ea-c8df-4b8f-b1d1-c16c80b638d2","trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657204259762,"user_tz":-120,"elapsed":190,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["MCC: 0.828\n"]}]},{"cell_type":"code","source":["#for i in range(len(flat_true_labels)):\n","#  print(flat_true_labels[i], flat_predictions[i])\n","print(len(flat_predictions))\n","unique, counts = np.unique(flat_predictions, return_counts=True)\n","#unique, counts = np.unique(flat_true_labels, return_counts=True)\n","dict(zip(unique, counts))\n","\n","# 0 = NonArg\n","# 1 = UnVerif\n","# 2 = Verif"],"metadata":{"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"aNo9EERdbNgM","executionInfo":{"status":"ok","timestamp":1657204261755,"user_tz":-120,"elapsed":177,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"a5abcd16-d088-4b94-dbb2-4da55e4c4e0d"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["638\n"]},{"output_type":"execute_result","data":{"text/plain":["{0: 515, 1: 123}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["def show_plot(cm, labels):\n","        ax= plt.subplot()\n","        sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n","\n","        # labels, title and ticks\n","        ax.set_xlabel('Predicted Labels');ax.set_ylabel('True Labels'); \n","        ax.set_title('Confusion Matrix'); \n","        ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n","        plt.show()"],"metadata":{"id":"cqkYrvmxdzQK","executionInfo":{"status":"ok","timestamp":1657204264691,"user_tz":-120,"elapsed":160,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import precision_recall_fscore_support\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","accuracy = accuracy_score(flat_true_labels, flat_predictions)\n","print(\"Accuracy: {}\".format(accuracy))\n","\n","macro = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='macro')\n","micro = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='micro')\n","weighted = precision_recall_fscore_support(flat_true_labels, flat_predictions, average='weighted')\n","\n","print(\"micro: precision = {} \\t recall = {} \\t f1 = {}\".format(micro[0], micro[1], micro[2]))\n","print(\"macro: precision = {} \\t recall = {} \\t f1 = {}\".format(macro[0], macro[1], macro[2]))\n","print(\"weighted: precision = {} \\t recall = {} \\t f1 = {}\".format(weighted[0], weighted[1], weighted[2]))\n","\n","per_class = precision_recall_fscore_support(flat_true_labels, flat_predictions, average=None, labels=[0, 1, 2])\n","print(\"Per Class:\")\n","print(\"NonArg: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][0], per_class[1][0], per_class[2][0]))\n","print(\"UnVerif: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][1], per_class[1][1], per_class[2][1]))\n","print(\"Verif: precision = {} \\t recall = {} \\t f1 = {}\".format(per_class[0][2], per_class[1][2], per_class[2][2]))\n","\n","cm = confusion_matrix(flat_true_labels, flat_predictions)\n","show_plot(cm, ['NonArg', 'UnVerif', 'Verif'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":503},"id":"4oDkwP-Md16R","executionInfo":{"status":"ok","timestamp":1657204271207,"user_tz":-120,"elapsed":592,"user":{"displayName":"Stijn Wijnen","userId":"04075930552500208092"}},"outputId":"ed9314e9-7f8f-4f18-93a7-b0a0f8075c7f"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.9467084639498433\n","micro: precision = 0.9467084639498433 \t recall = 0.9467084639498433 \t f1 = 0.9467084639498433\n","macro: precision = 0.9112952877101587 \t recall = 0.9164761737295586 \t f1 = 0.9138550006354048\n","weighted: precision = 0.9470698228910646 \t recall = 0.9467084639498433 \t f1 = 0.9468752327991552\n","Per Class:\n","NonArg: precision = 0.9689320388349515 \t recall = 0.965183752417795 \t f1 = 0.9670542635658915\n","UnVerif: precision = 0.8536585365853658 \t recall = 0.8677685950413223 \t f1 = 0.8606557377049181\n","Verif: precision = 0.0 \t recall = 0.0 \t f1 = 0.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}